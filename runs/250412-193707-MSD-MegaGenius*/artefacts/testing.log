2025-04-13 08:41:59,873 - INFO - Started logging to ./artefacts/testing.log
2025-04-13 08:41:59,873 - INFO - Config file: config.yaml
2025-04-13 08:41:59,873 - INFO - ==== Config file's contents ====
2025-04-13 08:41:59,874 - INFO - general:
2025-04-13 08:41:59,874 - INFO -   seed: 2024
2025-04-13 08:41:59,874 - INFO -   train: False
2025-04-13 08:41:59,874 - INFO -   dataset: lorentz63
2025-04-13 08:41:59,874 - INFO -   data_path: ./data/
2025-04-13 08:41:59,874 - INFO -   save_path: None
2025-04-13 08:41:59,874 - INFO - data:
2025-04-13 08:41:59,874 - INFO -   downsample_factor: 1
2025-04-13 08:41:59,875 - INFO -   resolution: [32, 32]
2025-04-13 08:41:59,875 - INFO -   traj_length: 1000
2025-04-13 08:41:59,875 - INFO - model:
2025-04-13 08:41:59,875 - INFO -   mlp_width_size: 48
2025-04-13 08:41:59,875 - INFO -   mlp_depth: 3
2025-04-13 08:41:59,875 - INFO -   activation: swish
2025-04-13 08:41:59,875 - INFO -   input_prev_data: False
2025-04-13 08:41:59,875 - INFO -   model_type: wsm-rnn
2025-04-13 08:41:59,876 - INFO -   nb_rnn_layers: 1
2025-04-13 08:41:59,876 - INFO -   weights_lim: None
2025-04-13 08:41:59,876 - INFO -   apply_tanh_uncertainty: False
2025-04-13 08:41:59,876 - INFO -   time_as_channel: False
2025-04-13 08:41:59,876 - INFO -   forcing_prob: 0.25
2025-04-13 08:41:59,876 - INFO -   noise_theta_init: None
2025-04-13 08:41:59,876 - INFO -   std_lower_bound: 0.0001
2025-04-13 08:41:59,876 - INFO - optimizer:
2025-04-13 08:41:59,877 - INFO -   init_lr: 1e-05
2025-04-13 08:41:59,877 - INFO -   gradients_lim: 1e-07
2025-04-13 08:41:59,877 - INFO -   on_plateau: {'factor': 0.5, 'min_scale': 0.01, 'patience': 20, 'cooldown': 0, 'rtol': 0.0001, 'accum_size': 50}
2025-04-13 08:41:59,877 - INFO - training:
2025-04-13 08:41:59,877 - INFO -   nb_epochs: 2500
2025-04-13 08:41:59,878 - INFO -   batch_size: 1024
2025-04-13 08:41:59,878 - INFO -   print_every: 10
2025-04-13 08:41:59,878 - INFO -   checkpoint_every: 10
2025-04-13 08:41:59,878 - INFO -   nb_recons_loss_steps: None
2025-04-13 08:41:59,878 - INFO -   use_nll_loss: False
2025-04-13 08:41:59,878 - INFO -   inference_start: 100
2025-04-13 08:42:00,606 - INFO - Images shape: (1024, 256, 2)
2025-04-13 08:42:00,607 - INFO - Labels shape: (1024,)
2025-04-13 08:42:00,607 - INFO - Seq length: 256
2025-04-13 08:42:00,607 - INFO - Data size: 2
2025-04-13 08:42:00,607 - INFO - Min/Max in the dataset: (-0.9856310478619905, 0.9731232709978324)
2025-04-13 08:42:00,608 - INFO - Number of batches:
2025-04-13 08:42:00,608 - INFO -   - Train: 20
2025-04-13 08:42:00,608 - INFO -   - Test: 20
2025-04-13 08:42:02,656 - INFO - Model loaded from ./model.eqx
2025-04-13 08:42:31,138 - INFO - Evaluation of MSE the test set:
2025-04-13 08:42:31,140 - INFO -     - Mean : 0.000137
2025-04-13 08:42:31,140 - INFO -     - Median : 0.000137
2025-04-13 08:42:31,141 - INFO -     - Min : 0.000118
2025-05-08 00:58:08,211 - INFO - Started logging to ./artefacts/testing.log
2025-05-08 00:58:08,211 - INFO - Config file: config.yaml
2025-05-08 00:58:08,212 - INFO - ==== Config file's contents ====
2025-05-08 00:58:08,212 - INFO - general:
2025-05-08 00:58:08,213 - INFO -   seed: 2024
2025-05-08 00:58:08,213 - INFO -   train: False
2025-05-08 00:58:08,213 - INFO -   dataset: lorentz63
2025-05-08 00:58:08,213 - INFO -   data_path: ./data/
2025-05-08 00:58:08,214 - INFO -   save_path: None
2025-05-08 00:58:08,214 - INFO - data:
2025-05-08 00:58:08,214 - INFO -   downsample_factor: 1
2025-05-08 00:58:08,215 - INFO -   resolution: [32, 32]
2025-05-08 00:58:08,215 - INFO -   traj_length: 1000
2025-05-08 00:58:08,215 - INFO - model:
2025-05-08 00:58:08,215 - INFO -   mlp_width_size: 48
2025-05-08 00:58:08,216 - INFO -   mlp_depth: 3
2025-05-08 00:58:08,216 - INFO -   activation: swish
2025-05-08 00:58:08,216 - INFO -   input_prev_data: False
2025-05-08 00:58:08,216 - INFO -   model_type: wsm-rnn
2025-05-08 00:58:08,217 - INFO -   nb_rnn_layers: 1
2025-05-08 00:58:08,217 - INFO -   weights_lim: None
2025-05-08 00:58:08,217 - INFO -   apply_tanh_uncertainty: False
2025-05-08 00:58:08,218 - INFO -   time_as_channel: False
2025-05-08 00:58:08,218 - INFO -   forcing_prob: 0.25
2025-05-08 00:58:08,218 - INFO -   noise_theta_init: None
2025-05-08 00:58:08,218 - INFO -   std_lower_bound: 0.0001
2025-05-08 00:58:08,219 - INFO - optimizer:
2025-05-08 00:58:08,219 - INFO -   init_lr: 1e-05
2025-05-08 00:58:08,219 - INFO -   gradients_lim: 1e-07
2025-05-08 00:58:08,220 - INFO -   on_plateau: {'factor': 0.5, 'min_scale': 0.01, 'patience': 20, 'cooldown': 0, 'rtol': 0.0001, 'accum_size': 50}
2025-05-08 00:58:08,220 - INFO - training:
2025-05-08 00:58:08,220 - INFO -   nb_epochs: 2500
2025-05-08 00:58:08,220 - INFO -   batch_size: 1024
2025-05-08 00:58:08,221 - INFO -   print_every: 10
2025-05-08 00:58:08,221 - INFO -   checkpoint_every: 10
2025-05-08 00:58:08,221 - INFO -   nb_recons_loss_steps: None
2025-05-08 00:58:08,221 - INFO -   use_nll_loss: False
2025-05-08 00:58:08,222 - INFO -   inference_start: 100
2025-05-08 00:59:07,763 - INFO - Started logging to ./artefacts/testing.log
2025-05-08 00:59:07,763 - INFO - Started logging to ./artefacts/testing.log
2025-05-08 00:59:07,763 - INFO - Config file: config.yaml
2025-05-08 00:59:07,763 - INFO - Config file: config.yaml
2025-05-08 00:59:07,764 - INFO - ==== Config file's contents ====
2025-05-08 00:59:07,764 - INFO - ==== Config file's contents ====
2025-05-08 00:59:07,764 - INFO - general:
2025-05-08 00:59:07,764 - INFO - general:
2025-05-08 00:59:07,765 - INFO -   seed: 2024
2025-05-08 00:59:07,765 - INFO -   seed: 2024
2025-05-08 00:59:07,765 - INFO -   train: False
2025-05-08 00:59:07,765 - INFO -   train: False
2025-05-08 00:59:07,765 - INFO -   dataset: lorentz63
2025-05-08 00:59:07,765 - INFO -   dataset: lorentz63
2025-05-08 00:59:07,765 - INFO -   data_path: ./data/
2025-05-08 00:59:07,765 - INFO -   data_path: ./data/
2025-05-08 00:59:07,766 - INFO -   save_path: None
2025-05-08 00:59:07,766 - INFO -   save_path: None
2025-05-08 00:59:07,766 - INFO - data:
2025-05-08 00:59:07,766 - INFO - data:
2025-05-08 00:59:07,766 - INFO -   downsample_factor: 1
2025-05-08 00:59:07,766 - INFO -   downsample_factor: 1
2025-05-08 00:59:07,766 - INFO -   resolution: [32, 32]
2025-05-08 00:59:07,766 - INFO -   resolution: [32, 32]
2025-05-08 00:59:07,767 - INFO -   traj_length: 1000
2025-05-08 00:59:07,767 - INFO -   traj_length: 1000
2025-05-08 00:59:07,767 - INFO - model:
2025-05-08 00:59:07,767 - INFO - model:
2025-05-08 00:59:07,767 - INFO -   mlp_width_size: 48
2025-05-08 00:59:07,767 - INFO -   mlp_width_size: 48
2025-05-08 00:59:07,767 - INFO -   mlp_depth: 3
2025-05-08 00:59:07,767 - INFO -   mlp_depth: 3
2025-05-08 00:59:07,768 - INFO -   activation: swish
2025-05-08 00:59:07,768 - INFO -   activation: swish
2025-05-08 00:59:07,768 - INFO -   input_prev_data: False
2025-05-08 00:59:07,768 - INFO -   input_prev_data: False
2025-05-08 00:59:07,768 - INFO -   model_type: wsm-rnn
2025-05-08 00:59:07,768 - INFO -   model_type: wsm-rnn
2025-05-08 00:59:07,769 - INFO -   nb_rnn_layers: 1
2025-05-08 00:59:07,769 - INFO -   nb_rnn_layers: 1
2025-05-08 00:59:07,769 - INFO -   weights_lim: None
2025-05-08 00:59:07,769 - INFO -   weights_lim: None
2025-05-08 00:59:07,769 - INFO -   apply_tanh_uncertainty: False
2025-05-08 00:59:07,769 - INFO -   apply_tanh_uncertainty: False
2025-05-08 00:59:07,769 - INFO -   time_as_channel: False
2025-05-08 00:59:07,769 - INFO -   time_as_channel: False
2025-05-08 00:59:07,770 - INFO -   forcing_prob: 0.25
2025-05-08 00:59:07,770 - INFO -   forcing_prob: 0.25
2025-05-08 00:59:07,770 - INFO -   noise_theta_init: None
2025-05-08 00:59:07,770 - INFO -   noise_theta_init: None
2025-05-08 00:59:07,770 - INFO -   std_lower_bound: 0.0001
2025-05-08 00:59:07,770 - INFO -   std_lower_bound: 0.0001
2025-05-08 00:59:07,770 - INFO - optimizer:
2025-05-08 00:59:07,770 - INFO - optimizer:
2025-05-08 00:59:07,770 - INFO -   init_lr: 1e-05
2025-05-08 00:59:07,770 - INFO -   init_lr: 1e-05
2025-05-08 00:59:07,771 - INFO -   gradients_lim: 1e-07
2025-05-08 00:59:07,771 - INFO -   gradients_lim: 1e-07
2025-05-08 00:59:07,771 - INFO -   on_plateau: {'factor': 0.5, 'min_scale': 0.01, 'patience': 20, 'cooldown': 0, 'rtol': 0.0001, 'accum_size': 50}
2025-05-08 00:59:07,771 - INFO -   on_plateau: {'factor': 0.5, 'min_scale': 0.01, 'patience': 20, 'cooldown': 0, 'rtol': 0.0001, 'accum_size': 50}
2025-05-08 00:59:07,771 - INFO - training:
2025-05-08 00:59:07,771 - INFO - training:
2025-05-08 00:59:07,772 - INFO -   nb_epochs: 2500
2025-05-08 00:59:07,772 - INFO -   nb_epochs: 2500
2025-05-08 00:59:07,772 - INFO -   batch_size: 1024
2025-05-08 00:59:07,772 - INFO -   batch_size: 1024
2025-05-08 00:59:07,772 - INFO -   print_every: 10
2025-05-08 00:59:07,772 - INFO -   print_every: 10
2025-05-08 00:59:07,772 - INFO -   checkpoint_every: 10
2025-05-08 00:59:07,772 - INFO -   checkpoint_every: 10
2025-05-08 00:59:07,772 - INFO -   nb_recons_loss_steps: None
2025-05-08 00:59:07,772 - INFO -   nb_recons_loss_steps: None
2025-05-08 00:59:07,773 - INFO -   use_nll_loss: False
2025-05-08 00:59:07,773 - INFO -   use_nll_loss: False
2025-05-08 00:59:07,773 - INFO -   inference_start: 100
2025-05-08 00:59:07,773 - INFO -   inference_start: 100
2025-05-08 00:59:47,890 - INFO - Started logging to ./artefacts/testing.log
2025-05-08 00:59:47,890 - INFO - Started logging to ./artefacts/testing.log
2025-05-08 00:59:47,890 - INFO - Started logging to ./artefacts/testing.log
2025-05-08 00:59:47,891 - INFO - Config file: config.yaml
2025-05-08 00:59:47,891 - INFO - Config file: config.yaml
2025-05-08 00:59:47,891 - INFO - Config file: config.yaml
2025-05-08 00:59:47,891 - INFO - ==== Config file's contents ====
2025-05-08 00:59:47,891 - INFO - ==== Config file's contents ====
2025-05-08 00:59:47,891 - INFO - ==== Config file's contents ====
2025-05-08 00:59:47,892 - INFO - general:
2025-05-08 00:59:47,892 - INFO - general:
2025-05-08 00:59:47,892 - INFO - general:
2025-05-08 00:59:47,893 - INFO -   seed: 2024
2025-05-08 00:59:47,893 - INFO -   seed: 2024
2025-05-08 00:59:47,893 - INFO -   seed: 2024
2025-05-08 00:59:47,893 - INFO -   train: False
2025-05-08 00:59:47,893 - INFO -   train: False
2025-05-08 00:59:47,893 - INFO -   train: False
2025-05-08 00:59:47,894 - INFO -   dataset: lorentz63
2025-05-08 00:59:47,894 - INFO -   dataset: lorentz63
2025-05-08 00:59:47,894 - INFO -   dataset: lorentz63
2025-05-08 00:59:47,894 - INFO -   data_path: ./data/
2025-05-08 00:59:47,894 - INFO -   data_path: ./data/
2025-05-08 00:59:47,894 - INFO -   data_path: ./data/
2025-05-08 00:59:47,895 - INFO -   save_path: None
2025-05-08 00:59:47,895 - INFO -   save_path: None
2025-05-08 00:59:47,895 - INFO -   save_path: None
2025-05-08 00:59:47,895 - INFO - data:
2025-05-08 00:59:47,895 - INFO - data:
2025-05-08 00:59:47,895 - INFO - data:
2025-05-08 00:59:47,896 - INFO -   downsample_factor: 1
2025-05-08 00:59:47,896 - INFO -   downsample_factor: 1
2025-05-08 00:59:47,896 - INFO -   downsample_factor: 1
2025-05-08 00:59:47,896 - INFO -   resolution: [32, 32]
2025-05-08 00:59:47,896 - INFO -   resolution: [32, 32]
2025-05-08 00:59:47,896 - INFO -   resolution: [32, 32]
2025-05-08 00:59:47,897 - INFO -   traj_length: 1000
2025-05-08 00:59:47,897 - INFO -   traj_length: 1000
2025-05-08 00:59:47,897 - INFO -   traj_length: 1000
2025-05-08 00:59:47,897 - INFO - model:
2025-05-08 00:59:47,897 - INFO - model:
2025-05-08 00:59:47,897 - INFO - model:
2025-05-08 00:59:47,897 - INFO -   mlp_width_size: 48
2025-05-08 00:59:47,897 - INFO -   mlp_width_size: 48
2025-05-08 00:59:47,897 - INFO -   mlp_width_size: 48
2025-05-08 00:59:47,898 - INFO -   mlp_depth: 3
2025-05-08 00:59:47,898 - INFO -   mlp_depth: 3
2025-05-08 00:59:47,898 - INFO -   mlp_depth: 3
2025-05-08 00:59:47,898 - INFO -   activation: swish
2025-05-08 00:59:47,898 - INFO -   activation: swish
2025-05-08 00:59:47,898 - INFO -   activation: swish
2025-05-08 00:59:47,899 - INFO -   input_prev_data: False
2025-05-08 00:59:47,899 - INFO -   input_prev_data: False
2025-05-08 00:59:47,899 - INFO -   input_prev_data: False
2025-05-08 00:59:47,899 - INFO -   model_type: wsm-rnn
2025-05-08 00:59:47,899 - INFO -   model_type: wsm-rnn
2025-05-08 00:59:47,899 - INFO -   model_type: wsm-rnn
2025-05-08 00:59:47,900 - INFO -   nb_rnn_layers: 1
2025-05-08 00:59:47,900 - INFO -   nb_rnn_layers: 1
2025-05-08 00:59:47,900 - INFO -   nb_rnn_layers: 1
2025-05-08 00:59:47,902 - INFO -   weights_lim: None
2025-05-08 00:59:47,902 - INFO -   weights_lim: None
2025-05-08 00:59:47,902 - INFO -   weights_lim: None
2025-05-08 00:59:47,902 - INFO -   apply_tanh_uncertainty: False
2025-05-08 00:59:47,902 - INFO -   apply_tanh_uncertainty: False
2025-05-08 00:59:47,902 - INFO -   apply_tanh_uncertainty: False
2025-05-08 00:59:47,902 - INFO -   time_as_channel: False
2025-05-08 00:59:47,902 - INFO -   time_as_channel: False
2025-05-08 00:59:47,902 - INFO -   time_as_channel: False
2025-05-08 00:59:47,903 - INFO -   forcing_prob: 0.25
2025-05-08 00:59:47,903 - INFO -   forcing_prob: 0.25
2025-05-08 00:59:47,903 - INFO -   forcing_prob: 0.25
2025-05-08 00:59:47,903 - INFO -   noise_theta_init: None
2025-05-08 00:59:47,903 - INFO -   noise_theta_init: None
2025-05-08 00:59:47,903 - INFO -   noise_theta_init: None
2025-05-08 00:59:47,903 - INFO -   std_lower_bound: 0.0001
2025-05-08 00:59:47,903 - INFO -   std_lower_bound: 0.0001
2025-05-08 00:59:47,903 - INFO -   std_lower_bound: 0.0001
2025-05-08 00:59:47,904 - INFO - optimizer:
2025-05-08 00:59:47,904 - INFO - optimizer:
2025-05-08 00:59:47,904 - INFO - optimizer:
2025-05-08 00:59:47,904 - INFO -   init_lr: 1e-05
2025-05-08 00:59:47,904 - INFO -   init_lr: 1e-05
2025-05-08 00:59:47,904 - INFO -   init_lr: 1e-05
2025-05-08 00:59:47,904 - INFO -   gradients_lim: 1e-07
2025-05-08 00:59:47,904 - INFO -   gradients_lim: 1e-07
2025-05-08 00:59:47,904 - INFO -   gradients_lim: 1e-07
2025-05-08 00:59:47,905 - INFO -   on_plateau: {'factor': 0.5, 'min_scale': 0.01, 'patience': 20, 'cooldown': 0, 'rtol': 0.0001, 'accum_size': 50}
2025-05-08 00:59:47,905 - INFO -   on_plateau: {'factor': 0.5, 'min_scale': 0.01, 'patience': 20, 'cooldown': 0, 'rtol': 0.0001, 'accum_size': 50}
2025-05-08 00:59:47,905 - INFO -   on_plateau: {'factor': 0.5, 'min_scale': 0.01, 'patience': 20, 'cooldown': 0, 'rtol': 0.0001, 'accum_size': 50}
2025-05-08 00:59:47,905 - INFO - training:
2025-05-08 00:59:47,905 - INFO - training:
2025-05-08 00:59:47,905 - INFO - training:
2025-05-08 00:59:47,905 - INFO -   nb_epochs: 2500
2025-05-08 00:59:47,905 - INFO -   nb_epochs: 2500
2025-05-08 00:59:47,905 - INFO -   nb_epochs: 2500
2025-05-08 00:59:47,906 - INFO -   batch_size: 1024
2025-05-08 00:59:47,906 - INFO -   batch_size: 1024
2025-05-08 00:59:47,906 - INFO -   batch_size: 1024
2025-05-08 00:59:47,906 - INFO -   print_every: 10
2025-05-08 00:59:47,906 - INFO -   print_every: 10
2025-05-08 00:59:47,906 - INFO -   print_every: 10
2025-05-08 00:59:47,906 - INFO -   checkpoint_every: 10
2025-05-08 00:59:47,906 - INFO -   checkpoint_every: 10
2025-05-08 00:59:47,906 - INFO -   checkpoint_every: 10
2025-05-08 00:59:47,906 - INFO -   nb_recons_loss_steps: None
2025-05-08 00:59:47,906 - INFO -   nb_recons_loss_steps: None
2025-05-08 00:59:47,906 - INFO -   nb_recons_loss_steps: None
2025-05-08 00:59:47,907 - INFO -   use_nll_loss: False
2025-05-08 00:59:47,907 - INFO -   use_nll_loss: False
2025-05-08 00:59:47,907 - INFO -   use_nll_loss: False
2025-05-08 00:59:47,907 - INFO -   inference_start: 100
2025-05-08 00:59:47,907 - INFO -   inference_start: 100
2025-05-08 00:59:47,907 - INFO -   inference_start: 100
2025-05-08 01:02:37,437 - INFO - Started logging to ./artefacts/testing.log
2025-05-08 01:02:37,437 - INFO - Config file: config.yaml
2025-05-08 01:02:37,437 - INFO - ==== Config file's contents ====
2025-05-08 01:02:37,437 - INFO - general:
2025-05-08 01:02:37,438 - INFO -   seed: 2024
2025-05-08 01:02:37,438 - INFO -   train: False
2025-05-08 01:02:37,438 - INFO -   dataset: lorentz63
2025-05-08 01:02:37,438 - INFO -   data_path: ./data/
2025-05-08 01:02:37,438 - INFO -   save_path: None
2025-05-08 01:02:37,438 - INFO - data:
2025-05-08 01:02:37,438 - INFO -   downsample_factor: 1
2025-05-08 01:02:37,438 - INFO -   resolution: [32, 32]
2025-05-08 01:02:37,439 - INFO -   traj_length: 1000
2025-05-08 01:02:37,439 - INFO - model:
2025-05-08 01:02:37,439 - INFO -   mlp_width_size: 48
2025-05-08 01:02:37,439 - INFO -   mlp_depth: 3
2025-05-08 01:02:37,439 - INFO -   activation: swish
2025-05-08 01:02:37,439 - INFO -   input_prev_data: False
2025-05-08 01:02:37,439 - INFO -   model_type: wsm-rnn
2025-05-08 01:02:37,439 - INFO -   nb_rnn_layers: 1
2025-05-08 01:02:37,439 - INFO -   weights_lim: None
2025-05-08 01:02:37,439 - INFO -   apply_tanh_uncertainty: False
2025-05-08 01:02:37,440 - INFO -   time_as_channel: False
2025-05-08 01:02:37,440 - INFO -   forcing_prob: 0.25
2025-05-08 01:02:37,440 - INFO -   noise_theta_init: None
2025-05-08 01:02:37,440 - INFO -   std_lower_bound: 0.0001
2025-05-08 01:02:37,440 - INFO - optimizer:
2025-05-08 01:02:37,441 - INFO -   init_lr: 1e-05
2025-05-08 01:02:37,441 - INFO -   gradients_lim: 1e-07
2025-05-08 01:02:37,441 - INFO -   on_plateau: {'factor': 0.5, 'min_scale': 0.01, 'patience': 20, 'cooldown': 0, 'rtol': 0.0001, 'accum_size': 50}
2025-05-08 01:02:37,441 - INFO - training:
2025-05-08 01:02:37,441 - INFO -   nb_epochs: 2500
2025-05-08 01:02:37,441 - INFO -   batch_size: 1024
2025-05-08 01:02:37,441 - INFO -   print_every: 10
2025-05-08 01:02:37,441 - INFO -   checkpoint_every: 10
2025-05-08 01:02:37,441 - INFO -   nb_recons_loss_steps: None
2025-05-08 01:02:37,442 - INFO -   use_nll_loss: False
2025-05-08 01:02:37,442 - INFO -   inference_start: 100
2025-05-08 01:02:38,430 - INFO - Images shape: (1024, 256, 2)
2025-05-08 01:02:38,431 - INFO - Labels shape: (1024,)
2025-05-08 01:02:38,431 - INFO - Seq length: 256
2025-05-08 01:02:38,431 - INFO - Data size: 2
2025-05-08 01:02:38,432 - INFO - Min/Max in the dataset: (-0.9856310478619905, 0.9731232709978324)
2025-05-08 01:02:38,432 - INFO - Number of batches:
2025-05-08 01:02:38,433 - INFO -   - Train: 20
2025-05-08 01:02:38,433 - INFO -   - Test: 20
2025-05-08 01:02:40,523 - INFO - Model loaded from ./model.eqx
2025-05-08 01:03:06,252 - INFO - Evaluation of MSE the test set:
2025-05-08 01:03:06,254 - INFO -     - Mean : 0.000137
2025-05-08 01:03:06,254 - INFO -     - Median : 0.000137
2025-05-08 01:03:06,254 - INFO -     - Min : 0.000118
2025-05-08 01:09:58,936 - INFO - Started logging to ./artefacts/testing.log
2025-05-08 01:09:58,936 - INFO - Config file: config.yaml
2025-05-08 01:09:58,936 - INFO - ==== Config file's contents ====
2025-05-08 01:09:58,937 - INFO - general:
2025-05-08 01:09:58,937 - INFO -   seed: 2024
2025-05-08 01:09:58,937 - INFO -   train: False
2025-05-08 01:09:58,937 - INFO -   dataset: lorentz63
2025-05-08 01:09:58,938 - INFO -   data_path: ./data/
2025-05-08 01:09:58,938 - INFO -   save_path: None
2025-05-08 01:09:58,938 - INFO - data:
2025-05-08 01:09:58,938 - INFO -   downsample_factor: 1
2025-05-08 01:09:58,938 - INFO -   resolution: [32, 32]
2025-05-08 01:09:58,938 - INFO -   traj_length: 1000
2025-05-08 01:09:58,939 - INFO - model:
2025-05-08 01:09:58,939 - INFO -   mlp_width_size: 48
2025-05-08 01:09:58,939 - INFO -   mlp_depth: 3
2025-05-08 01:09:58,939 - INFO -   activation: swish
2025-05-08 01:09:58,939 - INFO -   input_prev_data: False
2025-05-08 01:09:58,940 - INFO -   model_type: wsm-rnn
2025-05-08 01:09:58,940 - INFO -   nb_rnn_layers: 1
2025-05-08 01:09:58,940 - INFO -   weights_lim: None
2025-05-08 01:09:58,940 - INFO -   apply_tanh_uncertainty: False
2025-05-08 01:09:58,940 - INFO -   time_as_channel: False
2025-05-08 01:09:58,941 - INFO -   forcing_prob: 0.25
2025-05-08 01:09:58,941 - INFO -   noise_theta_init: None
2025-05-08 01:09:58,941 - INFO -   std_lower_bound: 0.0001
2025-05-08 01:09:58,941 - INFO - optimizer:
2025-05-08 01:09:58,941 - INFO -   init_lr: 1e-05
2025-05-08 01:09:58,941 - INFO -   gradients_lim: 1e-07
2025-05-08 01:09:58,941 - INFO -   on_plateau: {'factor': 0.5, 'min_scale': 0.01, 'patience': 20, 'cooldown': 0, 'rtol': 0.0001, 'accum_size': 50}
2025-05-08 01:09:58,941 - INFO - training:
2025-05-08 01:09:58,941 - INFO -   nb_epochs: 2500
2025-05-08 01:09:58,941 - INFO -   batch_size: 1024
2025-05-08 01:09:58,942 - INFO -   print_every: 10
2025-05-08 01:09:58,942 - INFO -   checkpoint_every: 10
2025-05-08 01:09:58,942 - INFO -   nb_recons_loss_steps: None
2025-05-08 01:09:58,943 - INFO -   use_nll_loss: False
2025-05-08 01:09:58,943 - INFO -   inference_start: 100
2025-05-08 01:09:59,903 - INFO - Images shape: (1024, 256, 2)
2025-05-08 01:09:59,903 - INFO - Labels shape: (1024,)
2025-05-08 01:09:59,904 - INFO - Seq length: 256
2025-05-08 01:09:59,904 - INFO - Data size: 2
2025-05-08 01:09:59,904 - INFO - Min/Max in the dataset: (-0.9856310478619905, 0.9731232709978324)
2025-05-08 01:09:59,905 - INFO - Number of batches:
2025-05-08 01:09:59,905 - INFO -   - Train: 20
2025-05-08 01:09:59,905 - INFO -   - Test: 20
2025-05-08 01:10:02,339 - INFO - Model loaded from ./model.eqx
2025-05-08 01:10:26,311 - INFO - Evaluation of MSE the test set:
2025-05-08 01:10:26,313 - INFO -     - Mean : 0.000137
2025-05-08 01:10:26,314 - INFO -     - Median : 0.000137
2025-05-08 01:10:26,314 - INFO -     - Min : 0.000118
2025-05-12 10:39:33,109 - INFO - Started logging to ./artefacts/testing.log
2025-05-12 10:39:33,112 - INFO - Config file: config.yaml
2025-05-12 10:39:33,113 - INFO - ==== Config file's contents ====
2025-05-12 10:39:33,113 - INFO - general:
2025-05-12 10:39:33,114 - INFO -   seed: 2024
2025-05-12 10:39:33,115 - INFO -   train: False
2025-05-12 10:39:33,115 - INFO -   dataset: lorentz63
2025-05-12 10:39:33,116 - INFO -   data_path: ./data/
2025-05-12 10:39:33,117 - INFO -   save_path: None
2025-05-12 10:39:33,117 - INFO - data:
2025-05-12 10:39:33,118 - INFO -   downsample_factor: 1
2025-05-12 10:39:33,119 - INFO -   resolution: [32, 32]
2025-05-12 10:39:33,120 - INFO -   traj_length: 1000
2025-05-12 10:39:33,122 - INFO - model:
2025-05-12 10:39:33,122 - INFO -   mlp_width_size: 48
2025-05-12 10:39:33,122 - INFO -   mlp_depth: 3
2025-05-12 10:39:33,123 - INFO -   activation: swish
2025-05-12 10:39:33,123 - INFO -   input_prev_data: False
2025-05-12 10:39:33,124 - INFO -   model_type: wsm-rnn
2025-05-12 10:39:33,124 - INFO -   nb_rnn_layers: 1
2025-05-12 10:39:33,124 - INFO -   weights_lim: None
2025-05-12 10:39:33,125 - INFO -   apply_tanh_uncertainty: False
2025-05-12 10:39:33,125 - INFO -   time_as_channel: False
2025-05-12 10:39:33,125 - INFO -   forcing_prob: 0.25
2025-05-12 10:39:33,125 - INFO -   noise_theta_init: None
2025-05-12 10:39:33,125 - INFO -   std_lower_bound: 0.0001
2025-05-12 10:39:33,125 - INFO - optimizer:
2025-05-12 10:39:33,125 - INFO -   init_lr: 1e-05
2025-05-12 10:39:33,126 - INFO -   gradients_lim: 1e-07
2025-05-12 10:39:33,126 - INFO -   on_plateau: {'factor': 0.5, 'min_scale': 0.01, 'patience': 20, 'cooldown': 0, 'rtol': 0.0001, 'accum_size': 50}
2025-05-12 10:39:33,126 - INFO - training:
2025-05-12 10:39:33,126 - INFO -   nb_epochs: 2500
2025-05-12 10:39:33,126 - INFO -   batch_size: 1024
2025-05-12 10:39:33,127 - INFO -   print_every: 10
2025-05-12 10:39:33,127 - INFO -   checkpoint_every: 10
2025-05-12 10:39:33,127 - INFO -   nb_recons_loss_steps: None
2025-05-12 10:39:33,128 - INFO -   use_nll_loss: False
2025-05-12 10:39:33,128 - INFO -   inference_start: 100
2025-05-12 10:39:35,975 - INFO - Images shape: (1024, 256, 2)
2025-05-12 10:39:35,976 - INFO - Labels shape: (1024,)
2025-05-12 10:39:35,976 - INFO - Seq length: 256
2025-05-12 10:39:35,977 - INFO - Data size: 2
2025-05-12 10:39:35,978 - INFO - Min/Max in the dataset: (-0.9856310478619905, 0.9731232709978324)
2025-05-12 10:39:35,978 - INFO - Number of batches:
2025-05-12 10:39:35,978 - INFO -   - Train: 20
2025-05-12 10:39:35,978 - INFO -   - Test: 20
2025-05-12 10:39:38,721 - INFO - Model loaded from ./model.eqx
2025-05-12 10:40:05,526 - INFO - Evaluation of MSE the test set:
2025-05-12 10:40:05,538 - INFO -     - Mean : 0.000137
2025-05-12 10:40:05,539 - INFO -     - Median : 0.000137
2025-05-12 10:40:05,540 - INFO -     - Min : 0.000118
2025-05-12 12:16:35,616 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 12:16:35,964 - INFO - Finetuning step 10 - Loss: 0.060037
2025-05-12 12:16:36,095 - INFO - Finetuning step 20 - Loss: 0.059854
2025-05-12 12:16:36,216 - INFO - Finetuning step 30 - Loss: 0.059829
2025-05-12 12:16:36,338 - INFO - Finetuning step 40 - Loss: 0.059802
2025-05-12 12:16:36,459 - INFO - Finetuning step 50 - Loss: 0.059770
2025-05-12 12:16:36,582 - INFO - Finetuning step 60 - Loss: 0.059738
2025-05-12 12:16:36,703 - INFO - Finetuning step 70 - Loss: 0.059703
2025-05-12 12:16:36,825 - INFO - Finetuning step 80 - Loss: 0.059660
2025-05-12 12:16:36,947 - INFO - Finetuning step 90 - Loss: 0.059608
2025-05-12 12:16:37,069 - INFO - Finetuning step 100 - Loss: 0.059542
2025-05-12 12:16:37,192 - INFO - Finetuning step 110 - Loss: 0.059457
2025-05-12 12:16:37,316 - INFO - Finetuning step 120 - Loss: 0.059344
2025-05-12 12:16:37,438 - INFO - Finetuning step 130 - Loss: 0.059191
2025-05-12 12:16:37,562 - INFO - Finetuning step 140 - Loss: 0.058985
2025-05-12 12:16:37,686 - INFO - Finetuning step 150 - Loss: 0.058706
2025-05-12 12:16:37,809 - INFO - Finetuning step 160 - Loss: 0.058332
2025-05-12 12:16:37,931 - INFO - Finetuning step 170 - Loss: 0.057838
2025-05-12 12:16:38,053 - INFO - Finetuning step 180 - Loss: 0.057209
2025-05-12 12:16:38,176 - INFO - Finetuning step 190 - Loss: 0.056464
2025-05-12 12:16:38,298 - INFO - Finetuning step 200 - Loss: 0.055713
2025-05-12 12:16:38,420 - INFO - Finetuning step 210 - Loss: 0.055218
2025-05-12 12:16:38,543 - INFO - Finetuning step 220 - Loss: 0.055069
2025-05-12 12:16:38,666 - INFO - Finetuning step 230 - Loss: 0.055046
2025-05-12 12:16:38,789 - INFO - Finetuning step 240 - Loss: 0.055030
2025-05-12 12:16:38,909 - INFO - Finetuning step 250 - Loss: 0.055016
2025-05-12 12:16:38,970 - INFO - Finetuning step 255 - Loss: 0.055011
2025-05-12 12:16:55,611 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 12:16:55,616 - INFO - Finetuning step 10 - Loss: 0.060037
2025-05-12 12:16:55,620 - INFO - Finetuning step 20 - Loss: 0.059854
2025-05-12 12:16:55,625 - INFO - Finetuning step 30 - Loss: 0.059829
2025-05-12 12:16:55,629 - INFO - Finetuning step 40 - Loss: 0.059802
2025-05-12 12:16:55,633 - INFO - Finetuning step 50 - Loss: 0.059769
2025-05-12 12:16:55,637 - INFO - Finetuning step 60 - Loss: 0.059738
2025-05-12 12:16:55,641 - INFO - Finetuning step 70 - Loss: 0.059702
2025-05-12 12:16:55,645 - INFO - Finetuning step 80 - Loss: 0.059660
2025-05-12 12:16:55,649 - INFO - Finetuning step 90 - Loss: 0.059608
2025-05-12 12:16:55,653 - INFO - Finetuning step 100 - Loss: 0.059542
2025-05-12 12:16:55,657 - INFO - Finetuning step 110 - Loss: 0.059457
2025-05-12 12:16:55,662 - INFO - Finetuning step 120 - Loss: 0.059344
2025-05-12 12:16:55,666 - INFO - Finetuning step 130 - Loss: 0.059191
2025-05-12 12:16:55,670 - INFO - Finetuning step 140 - Loss: 0.058985
2025-05-12 12:16:55,674 - INFO - Finetuning step 150 - Loss: 0.058707
2025-05-12 12:16:55,678 - INFO - Finetuning step 160 - Loss: 0.058332
2025-05-12 12:16:55,682 - INFO - Finetuning step 170 - Loss: 0.057838
2025-05-12 12:16:55,686 - INFO - Finetuning step 180 - Loss: 0.057210
2025-05-12 12:16:55,690 - INFO - Finetuning step 190 - Loss: 0.056463
2025-05-12 12:16:55,695 - INFO - Finetuning step 200 - Loss: 0.055713
2025-05-12 12:16:55,699 - INFO - Finetuning step 210 - Loss: 0.055218
2025-05-12 12:16:55,703 - INFO - Finetuning step 220 - Loss: 0.055069
2025-05-12 12:16:55,707 - INFO - Finetuning step 230 - Loss: 0.055046
2025-05-12 12:16:55,711 - INFO - Finetuning step 240 - Loss: 0.055028
2025-05-12 12:16:55,715 - INFO - Finetuning step 250 - Loss: 0.055014
2025-05-12 12:16:55,717 - INFO - Finetuning step 255 - Loss: 0.055009
2025-05-12 12:18:54,616 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 12:18:54,622 - INFO - Finetuning step 10 - Loss: 0.060037
2025-05-12 12:18:54,626 - INFO - Finetuning step 20 - Loss: 0.059854
2025-05-12 12:18:54,630 - INFO - Finetuning step 30 - Loss: 0.059829
2025-05-12 12:18:54,635 - INFO - Finetuning step 40 - Loss: 0.059802
2025-05-12 12:18:54,640 - INFO - Finetuning step 50 - Loss: 0.059769
2025-05-12 12:18:54,644 - INFO - Finetuning step 60 - Loss: 0.059738
2025-05-12 12:18:54,648 - INFO - Finetuning step 70 - Loss: 0.059702
2025-05-12 12:18:54,652 - INFO - Finetuning step 80 - Loss: 0.059660
2025-05-12 12:18:54,656 - INFO - Finetuning step 90 - Loss: 0.059608
2025-05-12 12:18:54,660 - INFO - Finetuning step 100 - Loss: 0.059542
2025-05-12 12:18:54,664 - INFO - Finetuning step 110 - Loss: 0.059457
2025-05-12 12:18:54,668 - INFO - Finetuning step 120 - Loss: 0.059344
2025-05-12 12:18:54,672 - INFO - Finetuning step 130 - Loss: 0.059191
2025-05-12 12:18:54,676 - INFO - Finetuning step 140 - Loss: 0.058985
2025-05-12 12:18:54,681 - INFO - Finetuning step 150 - Loss: 0.058707
2025-05-12 12:18:54,685 - INFO - Finetuning step 160 - Loss: 0.058332
2025-05-12 12:18:54,689 - INFO - Finetuning step 170 - Loss: 0.057838
2025-05-12 12:18:54,693 - INFO - Finetuning step 180 - Loss: 0.057210
2025-05-12 12:18:54,697 - INFO - Finetuning step 190 - Loss: 0.056463
2025-05-12 12:18:54,701 - INFO - Finetuning step 200 - Loss: 0.055713
2025-05-12 12:18:54,704 - INFO - Finetuning step 210 - Loss: 0.055218
2025-05-12 12:18:54,708 - INFO - Finetuning step 220 - Loss: 0.055069
2025-05-12 12:18:54,712 - INFO - Finetuning step 230 - Loss: 0.055046
2025-05-12 12:18:54,716 - INFO - Finetuning step 240 - Loss: 0.055028
2025-05-12 12:18:54,720 - INFO - Finetuning step 250 - Loss: 0.055014
2025-05-12 12:18:54,722 - INFO - Finetuning step 255 - Loss: 0.055009
2025-05-12 12:20:35,866 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 12:20:35,871 - INFO - Finetuning step 10 - Loss: 0.060037
2025-05-12 12:20:35,875 - INFO - Finetuning step 20 - Loss: 0.059854
2025-05-12 12:20:35,879 - INFO - Finetuning step 30 - Loss: 0.059829
2025-05-12 12:20:35,884 - INFO - Finetuning step 40 - Loss: 0.059802
2025-05-12 12:20:35,889 - INFO - Finetuning step 50 - Loss: 0.059769
2025-05-12 12:20:35,893 - INFO - Finetuning step 60 - Loss: 0.059738
2025-05-12 12:20:35,897 - INFO - Finetuning step 70 - Loss: 0.059702
2025-05-12 12:20:35,901 - INFO - Finetuning step 80 - Loss: 0.059660
2025-05-12 12:20:35,905 - INFO - Finetuning step 90 - Loss: 0.059608
2025-05-12 12:20:35,910 - INFO - Finetuning step 100 - Loss: 0.059542
2025-05-12 12:20:35,914 - INFO - Finetuning step 110 - Loss: 0.059457
2025-05-12 12:20:35,918 - INFO - Finetuning step 120 - Loss: 0.059344
2025-05-12 12:20:35,922 - INFO - Finetuning step 130 - Loss: 0.059191
2025-05-12 12:20:35,926 - INFO - Finetuning step 140 - Loss: 0.058985
2025-05-12 12:20:35,930 - INFO - Finetuning step 150 - Loss: 0.058707
2025-05-12 12:20:35,934 - INFO - Finetuning step 160 - Loss: 0.058332
2025-05-12 12:20:35,938 - INFO - Finetuning step 170 - Loss: 0.057838
2025-05-12 12:20:35,942 - INFO - Finetuning step 180 - Loss: 0.057210
2025-05-12 12:20:35,946 - INFO - Finetuning step 190 - Loss: 0.056463
2025-05-12 12:20:35,950 - INFO - Finetuning step 200 - Loss: 0.055713
2025-05-12 12:20:35,954 - INFO - Finetuning step 210 - Loss: 0.055218
2025-05-12 12:20:35,958 - INFO - Finetuning step 220 - Loss: 0.055069
2025-05-12 12:20:35,962 - INFO - Finetuning step 230 - Loss: 0.055046
2025-05-12 12:20:35,966 - INFO - Finetuning step 240 - Loss: 0.055028
2025-05-12 12:20:35,970 - INFO - Finetuning step 250 - Loss: 0.055014
2025-05-12 12:20:35,972 - INFO - Finetuning step 255 - Loss: 0.055009
2025-05-12 12:20:58,605 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 12:20:58,610 - INFO - Finetuning step 10 - Loss: 0.060037
2025-05-12 12:20:58,614 - INFO - Finetuning step 20 - Loss: 0.059854
2025-05-12 12:20:58,618 - INFO - Finetuning step 30 - Loss: 0.059829
2025-05-12 12:20:58,623 - INFO - Finetuning step 40 - Loss: 0.059802
2025-05-12 12:20:58,627 - INFO - Finetuning step 50 - Loss: 0.059769
2025-05-12 12:20:58,631 - INFO - Finetuning step 60 - Loss: 0.059738
2025-05-12 12:20:58,635 - INFO - Finetuning step 70 - Loss: 0.059702
2025-05-12 12:20:58,639 - INFO - Finetuning step 80 - Loss: 0.059660
2025-05-12 12:20:58,644 - INFO - Finetuning step 90 - Loss: 0.059608
2025-05-12 12:20:58,648 - INFO - Finetuning step 100 - Loss: 0.059542
2025-05-12 12:20:58,652 - INFO - Finetuning step 110 - Loss: 0.059457
2025-05-12 12:20:58,656 - INFO - Finetuning step 120 - Loss: 0.059344
2025-05-12 12:20:58,660 - INFO - Finetuning step 130 - Loss: 0.059191
2025-05-12 12:20:58,664 - INFO - Finetuning step 140 - Loss: 0.058985
2025-05-12 12:20:58,668 - INFO - Finetuning step 150 - Loss: 0.058707
2025-05-12 12:20:58,672 - INFO - Finetuning step 160 - Loss: 0.058332
2025-05-12 12:20:58,676 - INFO - Finetuning step 170 - Loss: 0.057838
2025-05-12 12:20:58,681 - INFO - Finetuning step 180 - Loss: 0.057210
2025-05-12 12:20:58,685 - INFO - Finetuning step 190 - Loss: 0.056463
2025-05-12 12:20:58,689 - INFO - Finetuning step 200 - Loss: 0.055713
2025-05-12 12:20:58,693 - INFO - Finetuning step 210 - Loss: 0.055218
2025-05-12 12:20:58,697 - INFO - Finetuning step 220 - Loss: 0.055069
2025-05-12 12:20:58,701 - INFO - Finetuning step 230 - Loss: 0.055046
2025-05-12 12:20:58,705 - INFO - Finetuning step 240 - Loss: 0.055028
2025-05-12 12:20:58,709 - INFO - Finetuning step 250 - Loss: 0.055014
2025-05-12 12:20:58,712 - INFO - Finetuning step 255 - Loss: 0.055009
2025-05-12 12:21:13,611 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 12:21:13,651 - INFO - Finetuning step 100 - Loss: 0.059542
2025-05-12 12:21:13,689 - INFO - Finetuning step 200 - Loss: 0.055713
2025-05-12 12:21:13,710 - INFO - Finetuning step 255 - Loss: 0.055009
2025-05-12 12:21:13,727 - INFO - Finetuning step 300 - Loss: 0.054967
2025-05-12 12:21:13,765 - INFO - Finetuning step 400 - Loss: 0.054845
2025-05-12 12:21:13,802 - INFO - Finetuning step 500 - Loss: 0.054664
2025-05-12 12:21:13,840 - INFO - Finetuning step 600 - Loss: 0.054324
2025-05-12 12:21:13,877 - INFO - Finetuning step 700 - Loss: 0.053347
2025-05-12 12:21:13,927 - INFO - Finetuning step 800 - Loss: 0.046336
2025-05-12 12:21:13,973 - INFO - Finetuning step 900 - Loss: 0.036410
2025-05-12 12:21:14,014 - INFO - Finetuning step 1000 - Loss: 0.031268
2025-05-12 12:21:14,053 - INFO - Finetuning step 1100 - Loss: 0.025179
2025-05-12 12:21:14,090 - INFO - Finetuning step 1200 - Loss: 0.017219
2025-05-12 12:21:14,128 - INFO - Finetuning step 1300 - Loss: 0.011317
2025-05-12 12:21:14,166 - INFO - Finetuning step 1400 - Loss: 0.008182
2025-05-12 12:21:14,204 - INFO - Finetuning step 1500 - Loss: 0.006330
2025-05-12 12:21:14,253 - INFO - Finetuning step 1600 - Loss: 0.005259
2025-05-12 12:21:14,292 - INFO - Finetuning step 1700 - Loss: 0.004739
2025-05-12 12:21:14,332 - INFO - Finetuning step 1800 - Loss: 0.004408
2025-05-12 12:21:14,370 - INFO - Finetuning step 1900 - Loss: 0.004157
2025-05-12 12:21:14,408 - INFO - Finetuning step 2000 - Loss: 0.003942
2025-05-12 12:21:14,446 - INFO - Finetuning step 2100 - Loss: 0.003769
2025-05-12 12:21:14,485 - INFO - Finetuning step 2200 - Loss: 0.003616
2025-05-12 12:21:14,524 - INFO - Finetuning step 2300 - Loss: 0.003458
2025-05-12 12:21:14,563 - INFO - Finetuning step 2400 - Loss: 0.003327
2025-05-12 12:21:14,602 - INFO - Finetuning step 2500 - Loss: 0.003197
2025-05-12 12:21:47,272 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 12:21:47,312 - INFO - Finetuning step 100 - Loss: 0.054933
2025-05-12 12:21:47,351 - INFO - Finetuning step 200 - Loss: 0.054145
2025-05-12 12:21:47,373 - INFO - Finetuning step 255 - Loss: 0.046610
2025-05-12 12:21:47,391 - INFO - Finetuning step 300 - Loss: 0.036929
2025-05-12 12:21:47,429 - INFO - Finetuning step 400 - Loss: 0.026134
2025-05-12 12:21:47,468 - INFO - Finetuning step 500 - Loss: 0.014186
2025-05-12 12:21:47,506 - INFO - Finetuning step 600 - Loss: 0.006858
2025-05-12 12:21:47,544 - INFO - Finetuning step 700 - Loss: 0.003830
2025-05-12 12:21:47,585 - INFO - Finetuning step 800 - Loss: 0.001386
2025-05-12 12:21:47,624 - INFO - Finetuning step 900 - Loss: 0.000708
2025-05-12 12:21:47,662 - INFO - Finetuning step 1000 - Loss: 0.000532
2025-05-12 12:21:47,699 - INFO - Finetuning step 1100 - Loss: 0.000443
2025-05-12 12:21:47,737 - INFO - Finetuning step 1200 - Loss: 0.000371
2025-05-12 12:21:47,775 - INFO - Finetuning step 1300 - Loss: 0.000321
2025-05-12 12:21:47,813 - INFO - Finetuning step 1400 - Loss: 0.000288
2025-05-12 12:21:47,850 - INFO - Finetuning step 1500 - Loss: 0.000430
2025-05-12 12:21:47,888 - INFO - Finetuning step 1600 - Loss: 0.000314
2025-05-12 12:21:47,926 - INFO - Finetuning step 1700 - Loss: 0.000239
2025-05-12 12:21:47,963 - INFO - Finetuning step 1800 - Loss: 0.000325
2025-05-12 12:21:48,001 - INFO - Finetuning step 1900 - Loss: 0.000214
2025-05-12 12:21:48,038 - INFO - Finetuning step 2000 - Loss: 0.000209
2025-05-12 12:21:48,075 - INFO - Finetuning step 2100 - Loss: 0.000197
2025-05-12 12:21:48,113 - INFO - Finetuning step 2200 - Loss: 0.000194
2025-05-12 12:21:48,151 - INFO - Finetuning step 2300 - Loss: 0.000174
2025-05-12 12:21:48,190 - INFO - Finetuning step 2400 - Loss: 0.000182
2025-05-12 12:21:48,229 - INFO - Finetuning step 2500 - Loss: 0.000429
2025-05-12 12:23:03,172 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 12:23:03,214 - INFO - Finetuning step 100 - Loss: 0.037943
2025-05-12 12:23:03,253 - INFO - Finetuning step 200 - Loss: 0.002452
2025-05-12 12:23:03,274 - INFO - Finetuning step 255 - Loss: 0.000731
2025-05-12 12:23:03,292 - INFO - Finetuning step 300 - Loss: 0.000380
2025-05-12 12:23:03,331 - INFO - Finetuning step 400 - Loss: 0.000249
2025-05-12 12:23:03,370 - INFO - Finetuning step 500 - Loss: 0.000248
2025-05-12 12:23:03,408 - INFO - Finetuning step 600 - Loss: 0.000275
2025-05-12 12:23:03,446 - INFO - Finetuning step 700 - Loss: 0.000200
2025-05-12 12:23:03,488 - INFO - Finetuning step 800 - Loss: 0.000219
2025-05-12 12:23:03,526 - INFO - Finetuning step 900 - Loss: 0.000145
2025-05-12 12:23:03,564 - INFO - Finetuning step 1000 - Loss: 0.000147
2025-05-12 12:23:03,602 - INFO - Finetuning step 1100 - Loss: 0.000125
2025-05-12 12:23:03,641 - INFO - Finetuning step 1200 - Loss: 0.000131
2025-05-12 12:23:03,679 - INFO - Finetuning step 1300 - Loss: 0.000153
2025-05-12 12:23:03,717 - INFO - Finetuning step 1400 - Loss: 0.000073
2025-05-12 12:23:03,755 - INFO - Finetuning step 1500 - Loss: 0.000202
2025-05-12 12:23:03,793 - INFO - Finetuning step 1600 - Loss: 0.000144
2025-05-12 12:23:03,831 - INFO - Finetuning step 1700 - Loss: 0.000087
2025-05-12 12:23:03,869 - INFO - Finetuning step 1800 - Loss: 0.000102
2025-05-12 12:23:03,906 - INFO - Finetuning step 1900 - Loss: 0.000057
2025-05-12 12:23:03,945 - INFO - Finetuning step 2000 - Loss: 0.000140
2025-05-12 12:23:03,982 - INFO - Finetuning step 2100 - Loss: 0.000205
2025-05-12 12:23:04,020 - INFO - Finetuning step 2200 - Loss: 0.000069
2025-05-12 12:23:04,057 - INFO - Finetuning step 2300 - Loss: 0.000066
2025-05-12 12:23:04,095 - INFO - Finetuning step 2400 - Loss: 0.000114
2025-05-12 12:23:04,133 - INFO - Finetuning step 2500 - Loss: 0.000058
2025-05-12 12:23:14,095 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 12:23:14,135 - INFO - Finetuning step 100 - Loss: 0.571320
2025-05-12 12:23:14,175 - INFO - Finetuning step 200 - Loss: 0.571320
2025-05-12 12:23:14,196 - INFO - Finetuning step 255 - Loss: 0.571320
2025-05-12 12:23:14,214 - INFO - Finetuning step 300 - Loss: 0.571320
2025-05-12 12:23:14,252 - INFO - Finetuning step 400 - Loss: 0.571320
2025-05-12 12:23:14,290 - INFO - Finetuning step 500 - Loss: 0.571320
2025-05-12 12:23:14,328 - INFO - Finetuning step 600 - Loss: 0.571320
2025-05-12 12:23:14,366 - INFO - Finetuning step 700 - Loss: 0.571320
2025-05-12 12:23:14,404 - INFO - Finetuning step 800 - Loss: 0.571320
2025-05-12 12:23:14,443 - INFO - Finetuning step 900 - Loss: 0.571320
2025-05-12 12:23:14,481 - INFO - Finetuning step 1000 - Loss: 0.571320
2025-05-12 12:23:14,519 - INFO - Finetuning step 1100 - Loss: 0.571320
2025-05-12 12:23:14,557 - INFO - Finetuning step 1200 - Loss: 0.571320
2025-05-12 12:23:14,595 - INFO - Finetuning step 1300 - Loss: 0.571320
2025-05-12 12:23:14,634 - INFO - Finetuning step 1400 - Loss: 0.571320
2025-05-12 12:23:14,672 - INFO - Finetuning step 1500 - Loss: 0.571320
2025-05-12 12:23:14,710 - INFO - Finetuning step 1600 - Loss: 0.571320
2025-05-12 12:23:14,748 - INFO - Finetuning step 1700 - Loss: 0.571320
2025-05-12 12:23:14,786 - INFO - Finetuning step 1800 - Loss: 0.571320
2025-05-12 12:23:14,824 - INFO - Finetuning step 1900 - Loss: 0.571320
2025-05-12 12:23:14,861 - INFO - Finetuning step 2000 - Loss: 0.571320
2025-05-12 12:23:14,899 - INFO - Finetuning step 2100 - Loss: 0.571320
2025-05-12 12:23:14,938 - INFO - Finetuning step 2200 - Loss: 0.571320
2025-05-12 12:23:14,977 - INFO - Finetuning step 2300 - Loss: 0.571320
2025-05-12 12:23:15,015 - INFO - Finetuning step 2400 - Loss: 0.571320
2025-05-12 12:23:15,053 - INFO - Finetuning step 2500 - Loss: 0.571320
2025-05-12 12:23:23,414 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 12:23:23,454 - INFO - Finetuning step 100 - Loss: 0.042018
2025-05-12 12:23:23,492 - INFO - Finetuning step 200 - Loss: 0.034662
2025-05-12 12:23:23,514 - INFO - Finetuning step 255 - Loss: 0.028009
2025-05-12 12:23:23,531 - INFO - Finetuning step 300 - Loss: 0.054241
2025-05-12 12:23:23,568 - INFO - Finetuning step 400 - Loss: 0.000259
2025-05-12 12:23:23,606 - INFO - Finetuning step 500 - Loss: 0.000140
2025-05-12 12:23:23,643 - INFO - Finetuning step 600 - Loss: 0.000221
2025-05-12 12:23:23,681 - INFO - Finetuning step 700 - Loss: 0.000367
2025-05-12 12:23:23,731 - INFO - Finetuning step 800 - Loss: 0.000108
2025-05-12 12:23:23,770 - INFO - Finetuning step 900 - Loss: 0.000106
2025-05-12 12:23:23,808 - INFO - Finetuning step 1000 - Loss: 0.000081
2025-05-12 12:23:23,846 - INFO - Finetuning step 1100 - Loss: 0.000102
2025-05-12 12:23:23,884 - INFO - Finetuning step 1200 - Loss: 0.000229
2025-05-12 12:23:23,923 - INFO - Finetuning step 1300 - Loss: 0.000065
2025-05-12 12:23:23,960 - INFO - Finetuning step 1400 - Loss: 0.000191
2025-05-12 12:23:23,998 - INFO - Finetuning step 1500 - Loss: 0.000212
2025-05-12 12:23:24,036 - INFO - Finetuning step 1600 - Loss: 0.000148
2025-05-12 12:23:24,074 - INFO - Finetuning step 1700 - Loss: 0.000161
2025-05-12 12:23:24,112 - INFO - Finetuning step 1800 - Loss: 0.000244
2025-05-12 12:23:24,150 - INFO - Finetuning step 1900 - Loss: 0.000482
2025-05-12 12:23:24,188 - INFO - Finetuning step 2000 - Loss: 0.000088
2025-05-12 12:23:24,227 - INFO - Finetuning step 2100 - Loss: 0.000093
2025-05-12 12:23:24,265 - INFO - Finetuning step 2200 - Loss: 0.000530
2025-05-12 12:23:24,303 - INFO - Finetuning step 2300 - Loss: 0.000135
2025-05-12 12:23:24,340 - INFO - Finetuning step 2400 - Loss: 0.000157
2025-05-12 12:23:24,379 - INFO - Finetuning step 2500 - Loss: 0.000408
2025-05-12 12:23:30,777 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 12:23:30,817 - INFO - Finetuning step 100 - Loss: 0.037943
2025-05-12 12:23:30,855 - INFO - Finetuning step 200 - Loss: 0.002452
2025-05-12 12:23:30,876 - INFO - Finetuning step 255 - Loss: 0.000731
2025-05-12 12:23:30,893 - INFO - Finetuning step 300 - Loss: 0.000380
2025-05-12 12:23:30,932 - INFO - Finetuning step 400 - Loss: 0.000249
2025-05-12 12:23:30,973 - INFO - Finetuning step 500 - Loss: 0.000248
2025-05-12 12:23:31,011 - INFO - Finetuning step 600 - Loss: 0.000275
2025-05-12 12:23:31,049 - INFO - Finetuning step 700 - Loss: 0.000200
2025-05-12 12:23:31,089 - INFO - Finetuning step 800 - Loss: 0.000219
2025-05-12 12:23:31,127 - INFO - Finetuning step 900 - Loss: 0.000145
2025-05-12 12:23:31,164 - INFO - Finetuning step 1000 - Loss: 0.000147
2025-05-12 12:23:31,202 - INFO - Finetuning step 1100 - Loss: 0.000125
2025-05-12 12:23:31,240 - INFO - Finetuning step 1200 - Loss: 0.000131
2025-05-12 12:23:31,277 - INFO - Finetuning step 1300 - Loss: 0.000153
2025-05-12 12:23:31,315 - INFO - Finetuning step 1400 - Loss: 0.000073
2025-05-12 12:23:31,353 - INFO - Finetuning step 1500 - Loss: 0.000202
2025-05-12 12:23:31,391 - INFO - Finetuning step 1600 - Loss: 0.000144
2025-05-12 12:23:31,428 - INFO - Finetuning step 1700 - Loss: 0.000087
2025-05-12 12:23:31,465 - INFO - Finetuning step 1800 - Loss: 0.000102
2025-05-12 12:23:31,502 - INFO - Finetuning step 1900 - Loss: 0.000057
2025-05-12 12:23:31,540 - INFO - Finetuning step 2000 - Loss: 0.000140
2025-05-12 12:23:31,577 - INFO - Finetuning step 2100 - Loss: 0.000205
2025-05-12 12:23:31,614 - INFO - Finetuning step 2200 - Loss: 0.000069
2025-05-12 12:23:31,651 - INFO - Finetuning step 2300 - Loss: 0.000066
2025-05-12 12:23:31,689 - INFO - Finetuning step 2400 - Loss: 0.000114
2025-05-12 12:23:31,726 - INFO - Finetuning step 2500 - Loss: 0.000058
2025-05-12 12:23:36,834 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 12:23:36,874 - INFO - Finetuning step 100 - Loss: 0.037943
2025-05-12 12:23:36,913 - INFO - Finetuning step 200 - Loss: 0.002452
2025-05-12 12:23:36,934 - INFO - Finetuning step 255 - Loss: 0.000731
2025-05-12 12:24:19,381 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 12:24:19,421 - INFO - Finetuning step 100 - Loss: 0.037943
2025-05-12 12:24:19,460 - INFO - Finetuning step 200 - Loss: 0.002452
2025-05-12 12:24:19,481 - INFO - Finetuning step 255 - Loss: 0.000731
2025-05-12 12:24:44,197 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 12:24:44,236 - INFO - Finetuning step 100 - Loss: 0.037943
2025-05-12 12:24:44,275 - INFO - Finetuning step 200 - Loss: 0.002452
2025-05-12 12:24:44,297 - INFO - Finetuning step 255 - Loss: 0.000731
2025-05-12 12:24:44,315 - INFO - Finetuning step 300 - Loss: 0.000380
2025-05-12 12:24:44,353 - INFO - Finetuning step 400 - Loss: 0.000249
2025-05-12 12:24:44,390 - INFO - Finetuning step 500 - Loss: 0.000248
2025-05-12 12:24:58,041 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 12:24:58,081 - INFO - Finetuning step 100 - Loss: 0.037943
2025-05-12 12:24:58,119 - INFO - Finetuning step 200 - Loss: 0.002452
2025-05-12 12:24:58,141 - INFO - Finetuning step 255 - Loss: 0.000731
2025-05-12 12:41:49,544 - INFO - Finetuning step 0 - Loss: 0.001423
2025-05-12 12:41:49,642 - INFO - Finetuning step 100 - Loss: 0.000193
2025-05-12 12:41:49,724 - INFO - Finetuning step 200 - Loss: 0.000000
2025-05-12 12:41:49,776 - INFO - Finetuning step 255 - Loss: 0.000000
2025-05-12 12:42:51,362 - INFO - Finetuning step 0 - Loss: 0.001423
2025-05-12 12:42:51,443 - INFO - Finetuning step 100 - Loss: 0.000193
2025-05-12 12:42:51,531 - INFO - Finetuning step 200 - Loss: 0.000000
2025-05-12 12:42:51,589 - INFO - Finetuning step 255 - Loss: 0.000000
2025-05-12 12:59:14,548 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 12:59:14,636 - INFO - Finetuning step 100 - Loss: 0.000008
2025-05-12 12:59:14,719 - INFO - Finetuning step 200 - Loss: 0.000000
2025-05-12 12:59:14,775 - INFO - Finetuning step 255 - Loss: 0.000000
2025-05-12 13:51:32,118 - INFO - Started logging to ./artefacts/testing.log
2025-05-12 13:51:32,119 - INFO - Config file: config.yaml
2025-05-12 13:51:32,120 - INFO - ==== Config file's contents ====
2025-05-12 13:51:32,121 - INFO - general:
2025-05-12 13:51:32,121 - INFO -   seed: 2024
2025-05-12 13:51:32,121 - INFO -   train: False
2025-05-12 13:51:32,122 - INFO -   dataset: lorentz63
2025-05-12 13:51:32,122 - INFO -   data_path: ./data/
2025-05-12 13:51:32,122 - INFO -   save_path: None
2025-05-12 13:51:32,122 - INFO - data:
2025-05-12 13:51:32,122 - INFO -   downsample_factor: 1
2025-05-12 13:51:32,123 - INFO -   resolution: [32, 32]
2025-05-12 13:51:32,123 - INFO -   traj_length: 1000
2025-05-12 13:51:32,123 - INFO - model:
2025-05-12 13:51:32,124 - INFO -   mlp_width_size: 48
2025-05-12 13:51:32,124 - INFO -   mlp_depth: 3
2025-05-12 13:51:32,124 - INFO -   activation: swish
2025-05-12 13:51:32,124 - INFO -   input_prev_data: False
2025-05-12 13:51:32,125 - INFO -   model_type: wsm-rnn
2025-05-12 13:51:32,125 - INFO -   nb_rnn_layers: 1
2025-05-12 13:51:32,125 - INFO -   weights_lim: None
2025-05-12 13:51:32,125 - INFO -   apply_tanh_uncertainty: False
2025-05-12 13:51:32,126 - INFO -   time_as_channel: False
2025-05-12 13:51:32,126 - INFO -   forcing_prob: 0.25
2025-05-12 13:51:32,126 - INFO -   noise_theta_init: None
2025-05-12 13:51:32,126 - INFO -   std_lower_bound: 0.0001
2025-05-12 13:51:32,127 - INFO - optimizer:
2025-05-12 13:51:32,127 - INFO -   init_lr: 1e-05
2025-05-12 13:51:32,127 - INFO -   gradients_lim: 1e-07
2025-05-12 13:51:32,128 - INFO -   on_plateau: {'factor': 0.5, 'min_scale': 0.01, 'patience': 20, 'cooldown': 0, 'rtol': 0.0001, 'accum_size': 50}
2025-05-12 13:51:32,128 - INFO - training:
2025-05-12 13:51:32,128 - INFO -   nb_epochs: 2500
2025-05-12 13:51:32,129 - INFO -   batch_size: 1024
2025-05-12 13:51:32,129 - INFO -   print_every: 10
2025-05-12 13:51:32,129 - INFO -   checkpoint_every: 10
2025-05-12 13:51:32,129 - INFO -   nb_recons_loss_steps: None
2025-05-12 13:51:32,129 - INFO -   use_nll_loss: False
2025-05-12 13:51:32,129 - INFO -   inference_start: 100
2025-05-12 13:51:32,800 - INFO - Images shape: (1024, 256, 2)
2025-05-12 13:51:32,801 - INFO - Labels shape: (1024,)
2025-05-12 13:51:32,801 - INFO - Seq length: 256
2025-05-12 13:51:32,801 - INFO - Data size: 2
2025-05-12 13:51:32,802 - INFO - Min/Max in the dataset: (-0.9856310478619905, 0.9731232709978324)
2025-05-12 13:51:32,802 - INFO - Number of batches:
2025-05-12 13:51:32,802 - INFO -   - Train: 20
2025-05-12 13:51:32,802 - INFO -   - Test: 20
2025-05-12 13:51:35,159 - INFO - Model loaded from ./model.eqx
2025-05-12 13:51:59,293 - INFO - Evaluation of MSE the test set:
2025-05-12 13:51:59,297 - INFO -     - Mean : 0.000137
2025-05-12 13:51:59,298 - INFO -     - Median : 0.000137
2025-05-12 13:51:59,299 - INFO -     - Min : 0.000118
2025-05-12 13:52:06,839 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 13:52:07,216 - INFO - Finetuning step 100 - Loss: 0.037943
2025-05-12 13:52:07,275 - INFO - Finetuning step 200 - Loss: 0.002452
2025-05-12 13:52:07,327 - INFO - Finetuning step 255 - Loss: 0.000731
2025-05-12 13:52:09,620 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 13:52:10,101 - INFO - Finetuning step 100 - Loss: 0.000008
2025-05-12 13:52:10,267 - INFO - Finetuning step 200 - Loss: 0.000000
2025-05-12 13:52:10,317 - INFO - Finetuning step 255 - Loss: 0.000000
2025-05-12 13:56:39,506 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 13:56:39,835 - INFO - Finetuning step 100 - Loss: 0.000008
2025-05-12 13:56:40,214 - INFO - Finetuning step 200 - Loss: 0.000000
2025-05-12 13:56:40,290 - INFO - Finetuning step 255 - Loss: 0.000000
2025-05-12 13:57:28,089 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 13:57:28,195 - INFO - Finetuning step 100 - Loss: 0.000008
2025-05-12 13:57:28,301 - INFO - Finetuning step 200 - Loss: 0.000000
2025-05-12 13:57:28,354 - INFO - Finetuning step 255 - Loss: 0.000000
2025-05-12 14:01:23,276 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 14:01:23,375 - INFO - Finetuning step 100 - Loss: 0.000008
2025-05-12 14:01:23,488 - INFO - Finetuning step 200 - Loss: 0.000000
2025-05-12 14:01:23,546 - INFO - Finetuning step 255 - Loss: 0.000000
2025-05-12 14:02:02,093 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 14:02:02,527 - INFO - Finetuning step 100 - Loss: 0.037943
2025-05-12 14:02:02,629 - INFO - Finetuning step 200 - Loss: 0.002452
2025-05-12 14:02:02,669 - INFO - Finetuning step 255 - Loss: 0.000731
2025-05-12 14:08:40,967 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 14:08:41,245 - INFO - Finetuning step 100 - Loss: 0.000008
2025-05-12 14:08:41,338 - INFO - Finetuning step 200 - Loss: 0.000000
2025-05-12 14:08:41,395 - INFO - Finetuning step 255 - Loss: 0.000000
2025-05-12 14:09:20,297 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 14:09:20,694 - INFO - Finetuning step 100 - Loss: 0.037943
2025-05-12 14:09:20,751 - INFO - Finetuning step 200 - Loss: 0.002452
2025-05-12 14:09:20,791 - INFO - Finetuning step 255 - Loss: 0.000731
2025-05-12 14:11:46,202 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 14:11:46,309 - INFO - Finetuning step 100 - Loss: 0.000008
2025-05-12 14:11:46,689 - INFO - Finetuning step 200 - Loss: 0.000000
2025-05-12 14:11:46,925 - INFO - Finetuning step 255 - Loss: 0.000000
2025-05-12 14:13:29,772 - INFO - Finetuning step 0 - Loss: 0.060482
2025-05-12 14:13:30,024 - INFO - Finetuning step 100 - Loss: 0.037943
2025-05-12 14:13:30,107 - INFO - Finetuning step 200 - Loss: 0.002452
2025-05-12 14:13:30,160 - INFO - Finetuning step 255 - Loss: 0.000731
2025-05-12 14:23:11,760 - INFO - Started logging to ./artefacts/testing.log
2025-05-12 14:23:11,761 - INFO - Config file: config.yaml
2025-05-12 14:23:11,761 - INFO - ==== Config file's contents ====
2025-05-12 14:23:11,761 - INFO - general:
2025-05-12 14:23:11,761 - INFO -   seed: 2024
2025-05-12 14:23:11,762 - INFO -   train: False
2025-05-12 14:23:11,762 - INFO -   dataset: lorentz63
2025-05-12 14:23:11,762 - INFO -   data_path: ./data/
2025-05-12 14:23:11,763 - INFO -   save_path: None
2025-05-12 14:23:11,763 - INFO - data:
2025-05-12 14:23:11,763 - INFO -   downsample_factor: 1
2025-05-12 14:23:11,763 - INFO -   resolution: [32, 32]
2025-05-12 14:23:11,763 - INFO -   traj_length: 1000
2025-05-12 14:23:11,764 - INFO - model:
2025-05-12 14:23:11,764 - INFO -   mlp_width_size: 48
2025-05-12 14:23:11,764 - INFO -   mlp_depth: 3
2025-05-12 14:23:11,765 - INFO -   activation: swish
2025-05-12 14:23:11,765 - INFO -   input_prev_data: False
2025-05-12 14:23:11,765 - INFO -   model_type: wsm-rnn
2025-05-12 14:23:11,766 - INFO -   nb_rnn_layers: 1
2025-05-12 14:23:11,766 - INFO -   weights_lim: None
2025-05-12 14:23:11,767 - INFO -   apply_tanh_uncertainty: False
2025-05-12 14:23:11,767 - INFO -   time_as_channel: False
2025-05-12 14:23:11,767 - INFO -   forcing_prob: 0.25
2025-05-12 14:23:11,768 - INFO -   noise_theta_init: None
2025-05-12 14:23:11,768 - INFO -   std_lower_bound: 0.0001
2025-05-12 14:23:11,769 - INFO - optimizer:
2025-05-12 14:23:11,769 - INFO -   init_lr: 1e-05
2025-05-12 14:23:11,769 - INFO -   gradients_lim: 1e-07
2025-05-12 14:23:11,770 - INFO -   on_plateau: {'factor': 0.5, 'min_scale': 0.01, 'patience': 20, 'cooldown': 0, 'rtol': 0.0001, 'accum_size': 50}
2025-05-12 14:23:11,770 - INFO - training:
2025-05-12 14:23:11,770 - INFO -   nb_epochs: 2500
2025-05-12 14:23:11,771 - INFO -   batch_size: 1024
2025-05-12 14:23:11,771 - INFO -   print_every: 10
2025-05-12 14:23:11,772 - INFO -   checkpoint_every: 10
2025-05-12 14:23:11,772 - INFO -   nb_recons_loss_steps: None
2025-05-12 14:23:11,772 - INFO -   use_nll_loss: False
2025-05-12 14:23:11,772 - INFO -   inference_start: 100
2025-05-12 14:23:12,614 - INFO - Images shape: (1024, 256, 2)
2025-05-12 14:23:12,615 - INFO - Labels shape: (1024,)
2025-05-12 14:23:12,615 - INFO - Seq length: 256
2025-05-12 14:23:12,615 - INFO - Data size: 2
2025-05-12 14:23:12,616 - INFO - Min/Max in the dataset: (-0.9856310478619905, 0.9731232709978324)
2025-05-12 14:23:12,616 - INFO - Number of batches:
2025-05-12 14:23:12,616 - INFO -   - Train: 20
2025-05-12 14:23:12,617 - INFO -   - Test: 20
2025-05-12 14:23:14,683 - INFO - Model loaded from ./model.eqx
2025-05-12 14:23:34,399 - INFO - Evaluation of MSE the test set:
2025-05-12 14:23:34,403 - INFO -     - Mean : 0.000137
2025-05-12 14:23:34,403 - INFO -     - Median : 0.000137
2025-05-12 14:23:34,404 - INFO -     - Min : 0.000118
2025-05-12 14:23:42,114 - INFO - Finetuning step 0 - Loss: 0.011561
2025-05-12 14:23:42,200 - INFO - Finetuning step 100 - Loss: 0.002701
2025-05-12 14:23:42,280 - INFO - Finetuning step 200 - Loss: 0.000275
2025-05-12 14:23:42,327 - INFO - Finetuning step 255 - Loss: 0.000435
2025-05-12 14:34:33,507 - INFO - Finetuning step 0 - Loss: 0.011561
2025-05-12 14:34:33,895 - INFO - Finetuning step 100 - Loss: 0.011362
2025-05-12 14:34:33,971 - INFO - Finetuning step 200 - Loss: 0.011305
2025-05-12 14:34:34,018 - INFO - Finetuning step 255 - Loss: 0.011292
2025-05-12 14:36:07,773 - INFO - Finetuning step 0 - Loss: 0.011561
2025-05-12 14:36:07,889 - INFO - Finetuning step 100 - Loss: 0.004078
2025-05-12 14:36:08,082 - INFO - Finetuning step 200 - Loss: 0.000231
2025-05-12 14:36:08,313 - INFO - Finetuning step 255 - Loss: 0.000388
2025-05-12 14:38:46,601 - INFO - Finetuning step 0 - Loss: 0.011561
2025-05-12 14:38:46,848 - INFO - Finetuning step 100 - Loss: 0.008355
2025-05-12 14:38:46,910 - INFO - Finetuning step 200 - Loss: 0.002543
2025-05-12 14:38:46,957 - INFO - Finetuning step 255 - Loss: 0.001095
2025-05-12 14:48:12,880 - INFO - Finetuning step 0 - Loss: 0.012723
2025-05-12 14:48:13,322 - INFO - Finetuning step 100 - Loss: 0.010660
2025-05-12 14:48:13,426 - INFO - Finetuning step 200 - Loss: 0.008066
2025-05-12 14:48:13,453 - INFO - Finetuning step 255 - Loss: 0.006752
2025-05-12 14:48:27,165 - INFO - Finetuning step 0 - Loss: 0.035210
2025-05-12 14:48:27,518 - INFO - Finetuning step 100 - Loss: 0.010394
2025-05-12 14:48:27,743 - INFO - Finetuning step 200 - Loss: 0.008261
2025-05-12 14:48:27,788 - INFO - Finetuning step 255 - Loss: 0.007921
2025-05-12 14:48:32,000 - INFO - Finetuning step 0 - Loss: 0.011561
2025-05-12 14:48:32,094 - INFO - Finetuning step 100 - Loss: 0.002701
2025-05-12 14:48:32,530 - INFO - Finetuning step 200 - Loss: 0.000275
2025-05-12 14:48:32,600 - INFO - Finetuning step 255 - Loss: 0.000435
2025-05-12 14:48:39,067 - INFO - Finetuning step 0 - Loss: 0.011668
2025-05-12 14:48:39,285 - INFO - Finetuning step 100 - Loss: 0.010421
2025-05-12 14:48:39,350 - INFO - Finetuning step 200 - Loss: 0.005954
2025-05-12 14:48:39,419 - INFO - Finetuning step 255 - Loss: 0.000845
2025-05-12 14:49:43,222 - INFO - Finetuning step 0 - Loss: 0.014312
2025-05-12 14:49:43,278 - INFO - Finetuning step 100 - Loss: 0.002508
2025-05-12 14:49:43,348 - INFO - Finetuning step 200 - Loss: 0.000054
2025-05-12 14:49:43,391 - INFO - Finetuning step 255 - Loss: 0.000064
2025-05-12 14:49:46,773 - INFO - Finetuning step 0 - Loss: 0.014312
2025-05-12 14:49:47,007 - INFO - Finetuning step 100 - Loss: 0.002508
2025-05-12 14:49:47,090 - INFO - Finetuning step 200 - Loss: 0.000054
2025-05-12 14:49:47,162 - INFO - Finetuning step 255 - Loss: 0.000064
2025-05-12 14:51:20,454 - INFO - Finetuning step 0 - Loss: 0.476584
2025-05-12 14:51:20,545 - INFO - Finetuning step 100 - Loss: 0.512153
2025-05-12 14:51:20,908 - INFO - Finetuning step 200 - Loss: 0.512153
2025-05-12 14:51:21,108 - INFO - Finetuning step 255 - Loss: 0.512153
2025-05-12 14:51:41,372 - INFO - Finetuning step 0 - Loss: 0.054258
2025-05-12 14:51:41,804 - INFO - Finetuning step 100 - Loss: 0.010100
2025-05-12 14:51:41,874 - INFO - Finetuning step 200 - Loss: 0.008201
2025-05-12 14:51:41,930 - INFO - Finetuning step 255 - Loss: 0.007781
2025-05-12 14:51:47,721 - INFO - Finetuning step 0 - Loss: 0.067920
2025-05-12 14:51:47,797 - INFO - Finetuning step 100 - Loss: 0.010323
2025-05-12 14:51:47,880 - INFO - Finetuning step 200 - Loss: 0.008237
2025-05-12 14:51:47,939 - INFO - Finetuning step 255 - Loss: 0.008154
2025-05-12 14:51:52,704 - INFO - Finetuning step 0 - Loss: 0.011561
2025-05-12 14:51:53,138 - INFO - Finetuning step 100 - Loss: 0.002701
2025-05-12 14:51:53,270 - INFO - Finetuning step 200 - Loss: 0.000275
2025-05-12 14:51:53,306 - INFO - Finetuning step 255 - Loss: 0.000435
2025-05-12 14:51:58,367 - INFO - Finetuning step 0 - Loss: 0.059379
2025-05-12 14:51:58,469 - INFO - Finetuning step 100 - Loss: 0.010448
2025-05-12 14:51:58,797 - INFO - Finetuning step 200 - Loss: 0.008220
2025-05-12 14:51:59,015 - INFO - Finetuning step 255 - Loss: 0.008035
2025-05-12 14:52:03,768 - INFO - Finetuning step 0 - Loss: 0.044244
2025-05-12 14:52:03,847 - INFO - Finetuning step 100 - Loss: 0.010509
2025-05-12 14:52:03,922 - INFO - Finetuning step 200 - Loss: 0.007873
2025-05-12 14:52:03,975 - INFO - Finetuning step 255 - Loss: 0.003431
2025-05-12 14:52:32,230 - INFO - Finetuning step 0 - Loss: 0.512153
2025-05-12 14:52:32,603 - INFO - Finetuning step 100 - Loss: 0.512153
2025-05-12 14:52:32,917 - INFO - Finetuning step 200 - Loss: 0.512153
2025-05-12 14:52:33,139 - INFO - Finetuning step 255 - Loss: 0.512153
2025-05-12 14:53:09,279 - INFO - Finetuning step 0 - Loss: 0.014004
2025-05-12 14:53:09,713 - INFO - Finetuning step 100 - Loss: 0.004978
2025-05-12 14:53:09,841 - INFO - Finetuning step 200 - Loss: 0.000836
2025-05-12 14:53:09,892 - INFO - Finetuning step 255 - Loss: 0.000611
2025-05-12 14:53:17,288 - INFO - Finetuning step 0 - Loss: 0.014004
2025-05-12 14:53:17,366 - INFO - Finetuning step 100 - Loss: 0.004978
2025-05-12 14:53:17,425 - INFO - Finetuning step 200 - Loss: 0.000836
2025-05-12 14:53:17,459 - INFO - Finetuning step 255 - Loss: 0.000611
2025-05-12 14:54:23,894 - INFO - Finetuning step 0 - Loss: 0.014004
2025-05-12 14:54:24,032 - INFO - Finetuning step 100 - Loss: 0.004978
2025-05-12 14:54:24,103 - INFO - Finetuning step 200 - Loss: 0.000836
2025-05-12 14:54:24,147 - INFO - Finetuning step 255 - Loss: 0.000611
2025-05-12 14:56:24,228 - INFO - Finetuning step 0 - Loss: 0.014004
2025-05-12 14:56:24,644 - INFO - Finetuning step 100 - Loss: 0.004978
2025-05-12 14:56:24,713 - INFO - Finetuning step 200 - Loss: 0.000836
2025-05-12 14:56:24,761 - INFO - Finetuning step 255 - Loss: 0.000611
2025-05-12 15:01:28,449 - INFO - Finetuning step 0 - Loss: 0.000936
2025-05-12 15:01:28,642 - INFO - Finetuning step 100 - Loss: 0.000152
2025-05-12 15:01:28,720 - INFO - Finetuning step 200 - Loss: 0.000046
2025-05-12 15:01:28,780 - INFO - Finetuning step 255 - Loss: 0.000026
2025-05-12 15:03:37,826 - INFO - Finetuning step 0 - Loss: 0.012367
2025-05-12 15:03:38,046 - INFO - Finetuning step 100 - Loss: 0.007223
2025-05-12 15:03:38,131 - INFO - Finetuning step 200 - Loss: 0.000494
2025-05-12 15:03:38,178 - INFO - Finetuning step 255 - Loss: 0.000137
2025-05-12 15:03:53,735 - INFO - Finetuning step 0 - Loss: 0.014112
2025-05-12 15:03:53,916 - INFO - Finetuning step 100 - Loss: 0.007556
2025-05-12 15:03:53,982 - INFO - Finetuning step 200 - Loss: 0.000903
2025-05-12 15:03:54,032 - INFO - Finetuning step 255 - Loss: 0.000093
2025-05-12 15:04:00,971 - INFO - Finetuning step 0 - Loss: 0.021203
2025-05-12 15:04:01,400 - INFO - Finetuning step 100 - Loss: 0.007148
2025-05-12 15:04:01,530 - INFO - Finetuning step 200 - Loss: 0.000619
2025-05-12 15:04:01,584 - INFO - Finetuning step 255 - Loss: 0.000177
2025-05-12 15:07:20,895 - INFO - Finetuning step 0 - Loss: 0.019537
2025-05-12 15:07:21,047 - INFO - Finetuning step 100 - Loss: 0.004691
2025-05-12 15:07:21,132 - INFO - Finetuning step 200 - Loss: 0.000942
2025-05-12 15:07:21,174 - INFO - Finetuning step 255 - Loss: 0.000167
2025-05-12 15:07:21,445 - INFO - Finetuning step 0 - Loss: 0.012427
2025-05-12 15:07:21,818 - INFO - Finetuning step 100 - Loss: 0.007010
2025-05-12 15:07:22,020 - INFO - Finetuning step 200 - Loss: 0.000126
2025-05-12 15:07:22,053 - INFO - Finetuning step 255 - Loss: 0.000587
2025-05-12 15:07:22,240 - INFO - Finetuning step 0 - Loss: 0.016677
2025-05-12 15:07:22,291 - INFO - Finetuning step 100 - Loss: 0.004848
2025-05-12 15:07:22,343 - INFO - Finetuning step 200 - Loss: 0.000734
2025-05-12 15:07:22,395 - INFO - Finetuning step 255 - Loss: 0.000088
2025-05-12 15:07:22,531 - INFO - Finetuning step 0 - Loss: 0.012719
2025-05-12 15:07:22,959 - INFO - Finetuning step 100 - Loss: 0.005407
2025-05-12 15:07:23,025 - INFO - Finetuning step 200 - Loss: 0.000335
2025-05-12 15:07:23,067 - INFO - Finetuning step 255 - Loss: 0.000125
2025-05-12 15:07:23,265 - INFO - Finetuning step 0 - Loss: 0.012406
2025-05-12 15:07:23,332 - INFO - Finetuning step 100 - Loss: 0.002765
2025-05-12 15:07:23,413 - INFO - Finetuning step 200 - Loss: 0.000827
2025-05-12 15:07:23,658 - INFO - Finetuning step 255 - Loss: 0.000261
2025-05-12 15:07:23,793 - INFO - Finetuning step 0 - Loss: 0.015828
2025-05-12 15:07:23,990 - INFO - Finetuning step 100 - Loss: 0.003259
2025-05-12 15:07:24,051 - INFO - Finetuning step 200 - Loss: 0.000372
2025-05-12 15:07:24,093 - INFO - Finetuning step 255 - Loss: 0.000126
2025-05-12 15:07:24,333 - INFO - Finetuning step 0 - Loss: 0.013753
2025-05-12 15:07:24,556 - INFO - Finetuning step 100 - Loss: 0.008096
2025-05-12 15:07:24,938 - INFO - Finetuning step 200 - Loss: 0.001238
2025-05-12 15:07:24,969 - INFO - Finetuning step 255 - Loss: 0.000181
2025-05-12 15:07:25,194 - INFO - Finetuning step 0 - Loss: 0.017697
2025-05-12 15:07:25,269 - INFO - Finetuning step 100 - Loss: 0.006392
2025-05-12 15:07:25,370 - INFO - Finetuning step 200 - Loss: 0.001144
2025-05-12 15:07:25,510 - INFO - Finetuning step 255 - Loss: 0.003490
2025-05-12 15:07:25,645 - INFO - Finetuning step 0 - Loss: 0.020078
2025-05-12 15:07:25,932 - INFO - Finetuning step 100 - Loss: 0.004735
2025-05-12 15:07:25,981 - INFO - Finetuning step 200 - Loss: 0.001022
2025-05-12 15:07:26,054 - INFO - Finetuning step 255 - Loss: 0.000170
2025-05-12 15:07:26,213 - INFO - Finetuning step 0 - Loss: 0.012569
2025-05-12 15:07:26,263 - INFO - Finetuning step 100 - Loss: 0.006548
2025-05-12 15:07:26,324 - INFO - Finetuning step 200 - Loss: 0.002231
2025-05-12 15:07:26,383 - INFO - Finetuning step 255 - Loss: 0.000829
2025-05-12 15:09:55,329 - INFO - Finetuning step 0 - Loss: 0.016023
2025-05-12 15:09:55,401 - INFO - Finetuning step 100 - Loss: 0.006071
2025-05-12 15:09:55,454 - INFO - Finetuning step 200 - Loss: 0.000762
2025-05-12 15:09:55,480 - INFO - Finetuning step 255 - Loss: 0.000112
2025-05-12 15:09:55,700 - INFO - Finetuning step 0 - Loss: 0.018095
2025-05-12 15:09:56,126 - INFO - Finetuning step 100 - Loss: 0.003044
2025-05-12 15:09:56,257 - INFO - Finetuning step 200 - Loss: 0.000123
2025-05-12 15:09:56,313 - INFO - Finetuning step 255 - Loss: 0.000037
2025-05-12 15:09:56,516 - INFO - Finetuning step 0 - Loss: 0.014035
2025-05-12 15:09:56,586 - INFO - Finetuning step 100 - Loss: 0.005080
2025-05-12 15:09:56,718 - INFO - Finetuning step 200 - Loss: 0.000195
2025-05-12 15:09:56,970 - INFO - Finetuning step 255 - Loss: 0.000084
2025-05-12 15:09:57,105 - INFO - Finetuning step 0 - Loss: 0.015842
2025-05-12 15:09:57,226 - INFO - Finetuning step 100 - Loss: 0.006144
2025-05-12 15:09:57,305 - INFO - Finetuning step 200 - Loss: 0.001792
2025-05-12 15:09:57,339 - INFO - Finetuning step 255 - Loss: 0.000526
2025-05-12 15:09:57,535 - INFO - Finetuning step 0 - Loss: 0.019844
2025-05-12 15:09:57,600 - INFO - Finetuning step 100 - Loss: 0.008549
2025-05-12 15:09:57,737 - INFO - Finetuning step 200 - Loss: 0.000773
2025-05-12 15:09:57,985 - INFO - Finetuning step 255 - Loss: 0.000560
2025-05-12 15:09:58,121 - INFO - Finetuning step 0 - Loss: 0.015289
2025-05-12 15:09:58,225 - INFO - Finetuning step 100 - Loss: 0.007276
2025-05-12 15:09:58,297 - INFO - Finetuning step 200 - Loss: 0.000445
2025-05-12 15:09:58,336 - INFO - Finetuning step 255 - Loss: 0.000020
2025-05-12 15:09:58,487 - INFO - Finetuning step 0 - Loss: 0.015165
2025-05-12 15:09:58,560 - INFO - Finetuning step 100 - Loss: 0.003014
2025-05-12 15:09:58,630 - INFO - Finetuning step 200 - Loss: 0.000897
2025-05-12 15:09:58,882 - INFO - Finetuning step 255 - Loss: 0.000267
2025-05-12 15:09:59,018 - INFO - Finetuning step 0 - Loss: 0.023129
2025-05-12 15:09:59,218 - INFO - Finetuning step 100 - Loss: 0.003945
2025-05-12 15:09:59,285 - INFO - Finetuning step 200 - Loss: 0.000334
2025-05-12 15:09:59,316 - INFO - Finetuning step 255 - Loss: 0.000175
2025-05-12 15:09:59,524 - INFO - Finetuning step 0 - Loss: 0.022256
2025-05-12 15:09:59,575 - INFO - Finetuning step 100 - Loss: 0.007651
2025-05-12 15:09:59,831 - INFO - Finetuning step 200 - Loss: 0.000850
2025-05-12 15:10:00,060 - INFO - Finetuning step 255 - Loss: 0.000658
2025-05-12 15:10:00,237 - INFO - Finetuning step 0 - Loss: 0.013024
2025-05-12 15:10:00,339 - INFO - Finetuning step 100 - Loss: 0.007286
2025-05-12 15:10:00,414 - INFO - Finetuning step 200 - Loss: 0.000181
2025-05-12 15:10:00,454 - INFO - Finetuning step 255 - Loss: 0.000063
2025-05-12 15:10:32,294 - INFO - Finetuning step 0 - Loss: 0.013096
2025-05-12 15:10:32,372 - INFO - Finetuning step 100 - Loss: 0.004604
2025-05-12 15:10:32,476 - INFO - Finetuning step 200 - Loss: 0.000505
2025-05-12 15:10:32,516 - INFO - Finetuning step 255 - Loss: 0.000188
2025-05-12 15:10:32,748 - INFO - Finetuning step 0 - Loss: 0.014918
2025-05-12 15:10:32,811 - INFO - Finetuning step 100 - Loss: 0.006417
2025-05-12 15:10:33,158 - INFO - Finetuning step 200 - Loss: 0.000715
2025-05-12 15:10:33,354 - INFO - Finetuning step 255 - Loss: 0.000030
2025-05-12 15:10:33,544 - INFO - Finetuning step 0 - Loss: 0.022251
2025-05-12 15:10:33,643 - INFO - Finetuning step 100 - Loss: 0.007963
2025-05-12 15:10:33,715 - INFO - Finetuning step 200 - Loss: 0.001534
2025-05-12 15:10:33,763 - INFO - Finetuning step 255 - Loss: 0.000830
2025-05-12 15:10:33,896 - INFO - Finetuning step 0 - Loss: 0.016127
2025-05-12 15:10:34,312 - INFO - Finetuning step 100 - Loss: 0.004153
2025-05-12 15:10:34,376 - INFO - Finetuning step 200 - Loss: 0.000909
2025-05-12 15:10:34,425 - INFO - Finetuning step 255 - Loss: 0.000938
2025-05-12 15:10:34,619 - INFO - Finetuning step 0 - Loss: 0.014688
2025-05-12 15:10:34,704 - INFO - Finetuning step 100 - Loss: 0.008096
2025-05-12 15:10:34,784 - INFO - Finetuning step 200 - Loss: 0.000119
2025-05-12 15:10:34,975 - INFO - Finetuning step 255 - Loss: 0.000030
2025-05-12 15:10:35,107 - INFO - Finetuning step 0 - Loss: 0.017658
2025-05-12 15:10:35,350 - INFO - Finetuning step 100 - Loss: 0.003037
2025-05-12 15:10:35,426 - INFO - Finetuning step 200 - Loss: 0.000025
2025-05-12 15:10:35,461 - INFO - Finetuning step 255 - Loss: 0.000031
2025-05-12 15:10:35,715 - INFO - Finetuning step 0 - Loss: 0.011770
2025-05-12 15:10:35,770 - INFO - Finetuning step 100 - Loss: 0.004844
2025-05-12 15:10:36,020 - INFO - Finetuning step 200 - Loss: 0.000732
2025-05-12 15:10:36,247 - INFO - Finetuning step 255 - Loss: 0.000147
2025-05-12 15:10:36,406 - INFO - Finetuning step 0 - Loss: 0.013274
2025-05-12 15:10:36,513 - INFO - Finetuning step 100 - Loss: 0.003222
2025-05-12 15:10:36,609 - INFO - Finetuning step 200 - Loss: 0.000380
2025-05-12 15:10:36,659 - INFO - Finetuning step 255 - Loss: 0.000133
2025-05-12 15:10:36,827 - INFO - Finetuning step 0 - Loss: 0.013489
2025-05-12 15:10:37,171 - INFO - Finetuning step 100 - Loss: 0.007444
2025-05-12 15:10:37,420 - INFO - Finetuning step 200 - Loss: 0.001868
2025-05-12 15:10:37,448 - INFO - Finetuning step 255 - Loss: 0.000707
2025-05-12 15:10:37,664 - INFO - Finetuning step 0 - Loss: 0.015683
2025-05-12 15:10:37,724 - INFO - Finetuning step 100 - Loss: 0.004992
2025-05-12 15:10:37,801 - INFO - Finetuning step 200 - Loss: 0.001488
2025-05-12 15:10:37,857 - INFO - Finetuning step 255 - Loss: 0.001027
2025-05-12 15:14:40,726 - INFO - Finetuning step 0 - Loss: 0.011561
2025-05-12 15:14:41,163 - INFO - Finetuning step 100 - Loss: 0.002701
2025-05-12 15:14:41,272 - INFO - Finetuning step 200 - Loss: 0.000275
2025-05-12 15:14:41,309 - INFO - Finetuning step 255 - Loss: 0.000435
2025-05-12 16:15:35,950 - INFO - Started logging to ./artefacts/testing.log
2025-05-12 16:15:35,950 - INFO - Config file: config.yaml
2025-05-12 16:15:35,950 - INFO - ==== Config file's contents ====
2025-05-12 16:15:35,951 - INFO - general:
2025-05-12 16:15:35,951 - INFO -   seed: 2024
2025-05-12 16:15:35,951 - INFO -   train: False
2025-05-12 16:15:35,951 - INFO -   dataset: lorentz63
2025-05-12 16:15:35,951 - INFO -   data_path: ./data/
2025-05-12 16:15:35,951 - INFO -   save_path: None
2025-05-12 16:15:35,952 - INFO - data:
2025-05-12 16:15:35,952 - INFO -   downsample_factor: 1
2025-05-12 16:15:35,952 - INFO -   resolution: [32, 32]
2025-05-12 16:15:35,952 - INFO -   traj_length: 1000
2025-05-12 16:15:35,952 - INFO - model:
2025-05-12 16:15:35,953 - INFO -   mlp_width_size: 48
2025-05-12 16:15:35,953 - INFO -   mlp_depth: 3
2025-05-12 16:15:35,953 - INFO -   activation: swish
2025-05-12 16:15:35,953 - INFO -   input_prev_data: False
2025-05-12 16:15:35,953 - INFO -   model_type: wsm-rnn
2025-05-12 16:15:35,953 - INFO -   nb_rnn_layers: 1
2025-05-12 16:15:35,954 - INFO -   weights_lim: None
2025-05-12 16:15:35,954 - INFO -   apply_tanh_uncertainty: False
2025-05-12 16:15:35,954 - INFO -   time_as_channel: False
2025-05-12 16:15:35,954 - INFO -   forcing_prob: 0.25
2025-05-12 16:15:35,954 - INFO -   noise_theta_init: None
2025-05-12 16:15:35,954 - INFO -   std_lower_bound: 0.0001
2025-05-12 16:15:35,955 - INFO - optimizer:
2025-05-12 16:15:35,955 - INFO -   init_lr: 1e-05
2025-05-12 16:15:35,955 - INFO -   gradients_lim: 1e-07
2025-05-12 16:15:35,955 - INFO -   on_plateau: {'factor': 0.5, 'min_scale': 0.01, 'patience': 20, 'cooldown': 0, 'rtol': 0.0001, 'accum_size': 50}
2025-05-12 16:15:35,955 - INFO - training:
2025-05-12 16:15:35,955 - INFO -   nb_epochs: 2500
2025-05-12 16:15:35,956 - INFO -   batch_size: 1024
2025-05-12 16:15:35,956 - INFO -   print_every: 10
2025-05-12 16:15:35,956 - INFO -   checkpoint_every: 10
2025-05-12 16:15:35,956 - INFO -   nb_recons_loss_steps: None
2025-05-12 16:15:35,957 - INFO -   use_nll_loss: False
2025-05-12 16:15:35,957 - INFO -   inference_start: 300
2025-05-12 16:15:36,542 - INFO - Images shape: (1024, 256, 2)
2025-05-12 16:15:36,543 - INFO - Labels shape: (1024,)
2025-05-12 16:15:36,543 - INFO - Seq length: 256
2025-05-12 16:15:36,543 - INFO - Data size: 2
2025-05-12 16:15:36,544 - INFO - Min/Max in the dataset: (-0.9856310478619905, 0.9731232709978324)
2025-05-12 16:15:36,544 - INFO - Number of batches:
2025-05-12 16:15:36,544 - INFO -   - Train: 20
2025-05-12 16:15:36,545 - INFO -   - Test: 20
2025-05-12 16:15:38,703 - INFO - Model loaded from ./model.eqx
2025-05-12 16:16:00,984 - INFO - Evaluation of MSE the test set:
2025-05-12 16:16:00,986 - INFO -     - Mean : 0.000137
2025-05-12 16:16:00,987 - INFO -     - Median : 0.000137
2025-05-12 16:16:00,987 - INFO -     - Min : 0.000118
2025-05-12 16:16:08,683 - INFO - Finetuning step 0 - Loss: 0.011561
2025-05-12 16:16:08,768 - INFO - Finetuning step 100 - Loss: 0.002701
2025-05-12 16:16:08,836 - INFO - Finetuning step 200 - Loss: 0.000275
2025-05-12 16:16:08,881 - INFO - Finetuning step 255 - Loss: 0.000435
2025-05-12 16:17:37,799 - INFO - Started logging to ./artefacts/testing.log
2025-05-12 16:17:37,799 - INFO - Started logging to ./artefacts/testing.log
2025-05-12 16:17:37,801 - INFO - Config file: config.yaml
2025-05-12 16:17:37,801 - INFO - Config file: config.yaml
2025-05-12 16:17:37,802 - INFO - ==== Config file's contents ====
2025-05-12 16:17:37,802 - INFO - ==== Config file's contents ====
2025-05-12 16:17:37,804 - INFO - general:
2025-05-12 16:17:37,804 - INFO - general:
2025-05-12 16:17:37,805 - INFO -   seed: 2024
2025-05-12 16:17:37,805 - INFO -   seed: 2024
2025-05-12 16:17:37,807 - INFO -   train: False
2025-05-12 16:17:37,807 - INFO -   train: False
2025-05-12 16:17:37,808 - INFO -   dataset: lorentz63
2025-05-12 16:17:37,808 - INFO -   dataset: lorentz63
2025-05-12 16:17:37,809 - INFO -   data_path: ./data/
2025-05-12 16:17:37,809 - INFO -   data_path: ./data/
2025-05-12 16:17:37,810 - INFO -   save_path: None
2025-05-12 16:17:37,810 - INFO -   save_path: None
2025-05-12 16:17:37,812 - INFO - data:
2025-05-12 16:17:37,812 - INFO - data:
2025-05-12 16:17:37,814 - INFO -   downsample_factor: 1
2025-05-12 16:17:37,814 - INFO -   downsample_factor: 1
2025-05-12 16:17:37,816 - INFO -   resolution: [32, 32]
2025-05-12 16:17:37,816 - INFO -   resolution: [32, 32]
2025-05-12 16:17:37,819 - INFO -   traj_length: 1000
2025-05-12 16:17:37,819 - INFO -   traj_length: 1000
2025-05-12 16:17:37,820 - INFO - model:
2025-05-12 16:17:37,820 - INFO - model:
2025-05-12 16:17:37,821 - INFO -   mlp_width_size: 48
2025-05-12 16:17:37,821 - INFO -   mlp_width_size: 48
2025-05-12 16:17:37,821 - INFO -   mlp_depth: 3
2025-05-12 16:17:37,821 - INFO -   mlp_depth: 3
2025-05-12 16:17:37,821 - INFO -   activation: swish
2025-05-12 16:17:37,821 - INFO -   activation: swish
2025-05-12 16:17:37,822 - INFO -   input_prev_data: False
2025-05-12 16:17:37,822 - INFO -   input_prev_data: False
2025-05-12 16:17:37,822 - INFO -   model_type: wsm-rnn
2025-05-12 16:17:37,822 - INFO -   model_type: wsm-rnn
2025-05-12 16:17:37,823 - INFO -   nb_rnn_layers: 1
2025-05-12 16:17:37,823 - INFO -   nb_rnn_layers: 1
2025-05-12 16:17:37,823 - INFO -   weights_lim: None
2025-05-12 16:17:37,823 - INFO -   weights_lim: None
2025-05-12 16:17:37,823 - INFO -   apply_tanh_uncertainty: False
2025-05-12 16:17:37,823 - INFO -   apply_tanh_uncertainty: False
2025-05-12 16:17:37,824 - INFO -   time_as_channel: False
2025-05-12 16:17:37,824 - INFO -   time_as_channel: False
2025-05-12 16:17:37,824 - INFO -   forcing_prob: 0.25
2025-05-12 16:17:37,824 - INFO -   forcing_prob: 0.25
2025-05-12 16:17:37,825 - INFO -   noise_theta_init: None
2025-05-12 16:17:37,825 - INFO -   noise_theta_init: None
2025-05-12 16:17:37,825 - INFO -   std_lower_bound: 0.0001
2025-05-12 16:17:37,825 - INFO -   std_lower_bound: 0.0001
2025-05-12 16:17:37,826 - INFO - optimizer:
2025-05-12 16:17:37,826 - INFO - optimizer:
2025-05-12 16:17:37,826 - INFO -   init_lr: 1e-05
2025-05-12 16:17:37,826 - INFO -   init_lr: 1e-05
2025-05-12 16:17:37,827 - INFO -   gradients_lim: 1e-07
2025-05-12 16:17:37,827 - INFO -   gradients_lim: 1e-07
2025-05-12 16:17:37,828 - INFO -   on_plateau: {'factor': 0.5, 'min_scale': 0.01, 'patience': 20, 'cooldown': 0, 'rtol': 0.0001, 'accum_size': 50}
2025-05-12 16:17:37,828 - INFO -   on_plateau: {'factor': 0.5, 'min_scale': 0.01, 'patience': 20, 'cooldown': 0, 'rtol': 0.0001, 'accum_size': 50}
2025-05-12 16:17:37,828 - INFO - training:
2025-05-12 16:17:37,828 - INFO - training:
2025-05-12 16:17:37,829 - INFO -   nb_epochs: 2500
2025-05-12 16:17:37,829 - INFO -   nb_epochs: 2500
2025-05-12 16:17:37,829 - INFO -   batch_size: 1024
2025-05-12 16:17:37,829 - INFO -   batch_size: 1024
2025-05-12 16:17:37,830 - INFO -   print_every: 10
2025-05-12 16:17:37,830 - INFO -   print_every: 10
2025-05-12 16:17:37,831 - INFO -   checkpoint_every: 10
2025-05-12 16:17:37,831 - INFO -   checkpoint_every: 10
2025-05-12 16:17:37,831 - INFO -   nb_recons_loss_steps: None
2025-05-12 16:17:37,831 - INFO -   nb_recons_loss_steps: None
2025-05-12 16:17:37,831 - INFO -   use_nll_loss: False
2025-05-12 16:17:37,831 - INFO -   use_nll_loss: False
2025-05-12 16:17:37,832 - INFO -   inference_start: 2
2025-05-12 16:17:37,832 - INFO -   inference_start: 2
2025-05-12 16:17:39,780 - INFO - Images shape: (1024, 256, 2)
2025-05-12 16:17:39,780 - INFO - Images shape: (1024, 256, 2)
2025-05-12 16:17:39,781 - INFO - Labels shape: (1024,)
2025-05-12 16:17:39,781 - INFO - Labels shape: (1024,)
2025-05-12 16:17:39,782 - INFO - Seq length: 256
2025-05-12 16:17:39,782 - INFO - Seq length: 256
2025-05-12 16:17:39,783 - INFO - Data size: 2
2025-05-12 16:17:39,783 - INFO - Data size: 2
2025-05-12 16:17:39,784 - INFO - Min/Max in the dataset: (-0.9856310478619905, 0.9731232709978324)
2025-05-12 16:17:39,784 - INFO - Min/Max in the dataset: (-0.9856310478619905, 0.9731232709978324)
2025-05-12 16:17:39,784 - INFO - Number of batches:
2025-05-12 16:17:39,784 - INFO - Number of batches:
2025-05-12 16:17:39,785 - INFO -   - Train: 20
2025-05-12 16:17:39,785 - INFO -   - Train: 20
2025-05-12 16:17:39,786 - INFO -   - Test: 20
2025-05-12 16:17:39,786 - INFO -   - Test: 20
2025-05-12 16:17:40,757 - INFO - Model loaded from ./model.eqx
2025-05-12 16:17:40,757 - INFO - Model loaded from ./model.eqx
2025-05-12 16:18:05,848 - INFO - Evaluation of MSE the test set:
2025-05-12 16:18:05,848 - INFO - Evaluation of MSE the test set:
2025-05-12 16:18:05,850 - INFO -     - Mean : 0.000137
2025-05-12 16:18:05,850 - INFO -     - Mean : 0.000137
2025-05-12 16:18:05,851 - INFO -     - Median : 0.000137
2025-05-12 16:18:05,851 - INFO -     - Median : 0.000137
2025-05-12 16:18:05,852 - INFO -     - Min : 0.000118
2025-05-12 16:18:05,852 - INFO -     - Min : 0.000118
2025-05-12 16:18:12,007 - INFO - Finetuning step 0 - Loss: 0.011561
2025-05-12 16:18:12,007 - INFO - Finetuning step 0 - Loss: 0.011561
2025-05-12 16:18:12,070 - INFO - Finetuning step 100 - Loss: 0.002701
2025-05-12 16:18:12,070 - INFO - Finetuning step 100 - Loss: 0.002701
2025-05-12 16:18:12,156 - INFO - Finetuning step 200 - Loss: 0.000275
2025-05-12 16:18:12,156 - INFO - Finetuning step 200 - Loss: 0.000275
2025-05-12 16:18:12,380 - INFO - Finetuning step 255 - Loss: 0.000435
2025-05-12 16:18:12,380 - INFO - Finetuning step 255 - Loss: 0.000435
2025-05-12 16:22:46,629 - INFO - Started logging to ./artefacts/testing.log
2025-05-12 16:22:46,629 - INFO - Started logging to ./artefacts/testing.log
2025-05-12 16:22:46,629 - INFO - Started logging to ./artefacts/testing.log
2025-05-12 16:22:46,630 - INFO - Config file: config.yaml
2025-05-12 16:22:46,630 - INFO - Config file: config.yaml
2025-05-12 16:22:46,630 - INFO - Config file: config.yaml
2025-05-12 16:22:46,630 - INFO - ==== Config file's contents ====
2025-05-12 16:22:46,630 - INFO - ==== Config file's contents ====
2025-05-12 16:22:46,630 - INFO - ==== Config file's contents ====
2025-05-12 16:22:46,631 - INFO - general:
2025-05-12 16:22:46,631 - INFO - general:
2025-05-12 16:22:46,631 - INFO - general:
2025-05-12 16:22:46,632 - INFO -   seed: 2024
2025-05-12 16:22:46,632 - INFO -   seed: 2024
2025-05-12 16:22:46,632 - INFO -   seed: 2024
2025-05-12 16:22:46,632 - INFO -   train: False
2025-05-12 16:22:46,632 - INFO -   train: False
2025-05-12 16:22:46,632 - INFO -   train: False
2025-05-12 16:22:46,634 - INFO -   dataset: lorentz63
2025-05-12 16:22:46,634 - INFO -   dataset: lorentz63
2025-05-12 16:22:46,634 - INFO -   dataset: lorentz63
2025-05-12 16:22:46,634 - INFO -   data_path: ./data/
2025-05-12 16:22:46,634 - INFO -   data_path: ./data/
2025-05-12 16:22:46,634 - INFO -   data_path: ./data/
2025-05-12 16:22:46,635 - INFO -   save_path: None
2025-05-12 16:22:46,635 - INFO -   save_path: None
2025-05-12 16:22:46,635 - INFO -   save_path: None
2025-05-12 16:22:46,636 - INFO - data:
2025-05-12 16:22:46,636 - INFO - data:
2025-05-12 16:22:46,636 - INFO - data:
2025-05-12 16:22:46,637 - INFO -   downsample_factor: 1
2025-05-12 16:22:46,637 - INFO -   downsample_factor: 1
2025-05-12 16:22:46,637 - INFO -   downsample_factor: 1
2025-05-12 16:22:46,637 - INFO -   resolution: [32, 32]
2025-05-12 16:22:46,637 - INFO -   resolution: [32, 32]
2025-05-12 16:22:46,637 - INFO -   resolution: [32, 32]
2025-05-12 16:22:46,638 - INFO -   traj_length: 1000
2025-05-12 16:22:46,638 - INFO -   traj_length: 1000
2025-05-12 16:22:46,638 - INFO -   traj_length: 1000
2025-05-12 16:22:46,639 - INFO - model:
2025-05-12 16:22:46,639 - INFO - model:
2025-05-12 16:22:46,639 - INFO - model:
2025-05-12 16:22:46,639 - INFO -   mlp_width_size: 48
2025-05-12 16:22:46,639 - INFO -   mlp_width_size: 48
2025-05-12 16:22:46,639 - INFO -   mlp_width_size: 48
2025-05-12 16:22:46,640 - INFO -   mlp_depth: 3
2025-05-12 16:22:46,640 - INFO -   mlp_depth: 3
2025-05-12 16:22:46,640 - INFO -   mlp_depth: 3
2025-05-12 16:22:46,641 - INFO -   activation: swish
2025-05-12 16:22:46,641 - INFO -   activation: swish
2025-05-12 16:22:46,641 - INFO -   activation: swish
2025-05-12 16:22:46,642 - INFO -   input_prev_data: False
2025-05-12 16:22:46,642 - INFO -   input_prev_data: False
2025-05-12 16:22:46,642 - INFO -   input_prev_data: False
2025-05-12 16:22:46,642 - INFO -   model_type: wsm-rnn
2025-05-12 16:22:46,642 - INFO -   model_type: wsm-rnn
2025-05-12 16:22:46,642 - INFO -   model_type: wsm-rnn
2025-05-12 16:22:46,643 - INFO -   nb_rnn_layers: 1
2025-05-12 16:22:46,643 - INFO -   nb_rnn_layers: 1
2025-05-12 16:22:46,643 - INFO -   nb_rnn_layers: 1
2025-05-12 16:22:46,644 - INFO -   weights_lim: None
2025-05-12 16:22:46,644 - INFO -   weights_lim: None
2025-05-12 16:22:46,644 - INFO -   weights_lim: None
2025-05-12 16:22:46,645 - INFO -   apply_tanh_uncertainty: False
2025-05-12 16:22:46,645 - INFO -   apply_tanh_uncertainty: False
2025-05-12 16:22:46,645 - INFO -   apply_tanh_uncertainty: False
2025-05-12 16:22:46,646 - INFO -   time_as_channel: False
2025-05-12 16:22:46,646 - INFO -   time_as_channel: False
2025-05-12 16:22:46,646 - INFO -   time_as_channel: False
2025-05-12 16:22:46,646 - INFO -   forcing_prob: 0.25
2025-05-12 16:22:46,646 - INFO -   forcing_prob: 0.25
2025-05-12 16:22:46,646 - INFO -   forcing_prob: 0.25
2025-05-12 16:22:46,647 - INFO -   noise_theta_init: None
2025-05-12 16:22:46,647 - INFO -   noise_theta_init: None
2025-05-12 16:22:46,647 - INFO -   noise_theta_init: None
2025-05-12 16:22:46,648 - INFO -   std_lower_bound: 0.0001
2025-05-12 16:22:46,648 - INFO -   std_lower_bound: 0.0001
2025-05-12 16:22:46,648 - INFO -   std_lower_bound: 0.0001
2025-05-12 16:22:46,649 - INFO - optimizer:
2025-05-12 16:22:46,649 - INFO - optimizer:
2025-05-12 16:22:46,649 - INFO - optimizer:
2025-05-12 16:22:46,649 - INFO -   init_lr: 1e-05
2025-05-12 16:22:46,649 - INFO -   init_lr: 1e-05
2025-05-12 16:22:46,649 - INFO -   init_lr: 1e-05
2025-05-12 16:22:46,649 - INFO -   gradients_lim: 1e-07
2025-05-12 16:22:46,649 - INFO -   gradients_lim: 1e-07
2025-05-12 16:22:46,649 - INFO -   gradients_lim: 1e-07
2025-05-12 16:22:46,650 - INFO -   on_plateau: {'factor': 0.5, 'min_scale': 0.01, 'patience': 20, 'cooldown': 0, 'rtol': 0.0001, 'accum_size': 50}
2025-05-12 16:22:46,650 - INFO -   on_plateau: {'factor': 0.5, 'min_scale': 0.01, 'patience': 20, 'cooldown': 0, 'rtol': 0.0001, 'accum_size': 50}
2025-05-12 16:22:46,650 - INFO -   on_plateau: {'factor': 0.5, 'min_scale': 0.01, 'patience': 20, 'cooldown': 0, 'rtol': 0.0001, 'accum_size': 50}
2025-05-12 16:22:46,650 - INFO - training:
2025-05-12 16:22:46,650 - INFO - training:
2025-05-12 16:22:46,650 - INFO - training:
2025-05-12 16:22:46,651 - INFO -   nb_epochs: 2500
2025-05-12 16:22:46,651 - INFO -   nb_epochs: 2500
2025-05-12 16:22:46,651 - INFO -   nb_epochs: 2500
2025-05-12 16:22:46,651 - INFO -   batch_size: 1024
2025-05-12 16:22:46,651 - INFO -   batch_size: 1024
2025-05-12 16:22:46,651 - INFO -   batch_size: 1024
2025-05-12 16:22:46,652 - INFO -   print_every: 10
2025-05-12 16:22:46,652 - INFO -   print_every: 10
2025-05-12 16:22:46,652 - INFO -   print_every: 10
2025-05-12 16:22:46,652 - INFO -   checkpoint_every: 10
2025-05-12 16:22:46,652 - INFO -   checkpoint_every: 10
2025-05-12 16:22:46,652 - INFO -   checkpoint_every: 10
2025-05-12 16:22:46,653 - INFO -   nb_recons_loss_steps: None
2025-05-12 16:22:46,653 - INFO -   nb_recons_loss_steps: None
2025-05-12 16:22:46,653 - INFO -   nb_recons_loss_steps: None
2025-05-12 16:22:46,653 - INFO -   use_nll_loss: False
2025-05-12 16:22:46,653 - INFO -   use_nll_loss: False
2025-05-12 16:22:46,653 - INFO -   use_nll_loss: False
2025-05-12 16:22:46,654 - INFO -   inference_start: 100
2025-05-12 16:22:46,654 - INFO -   inference_start: 100
2025-05-12 16:22:46,654 - INFO -   inference_start: 100
2025-05-12 16:22:47,957 - INFO - Images shape: (1024, 256, 2)
2025-05-12 16:22:47,957 - INFO - Images shape: (1024, 256, 2)
2025-05-12 16:22:47,957 - INFO - Images shape: (1024, 256, 2)
2025-05-12 16:22:47,958 - INFO - Labels shape: (1024,)
2025-05-12 16:22:47,958 - INFO - Labels shape: (1024,)
2025-05-12 16:22:47,958 - INFO - Labels shape: (1024,)
2025-05-12 16:22:47,959 - INFO - Seq length: 256
2025-05-12 16:22:47,959 - INFO - Seq length: 256
2025-05-12 16:22:47,959 - INFO - Seq length: 256
2025-05-12 16:22:47,959 - INFO - Data size: 2
2025-05-12 16:22:47,959 - INFO - Data size: 2
2025-05-12 16:22:47,959 - INFO - Data size: 2
2025-05-12 16:22:47,960 - INFO - Min/Max in the dataset: (-0.9856310478619905, 0.9731232709978324)
2025-05-12 16:22:47,960 - INFO - Min/Max in the dataset: (-0.9856310478619905, 0.9731232709978324)
2025-05-12 16:22:47,960 - INFO - Min/Max in the dataset: (-0.9856310478619905, 0.9731232709978324)
2025-05-12 16:22:47,961 - INFO - Number of batches:
2025-05-12 16:22:47,961 - INFO - Number of batches:
2025-05-12 16:22:47,961 - INFO - Number of batches:
2025-05-12 16:22:47,961 - INFO -   - Train: 20
2025-05-12 16:22:47,961 - INFO -   - Train: 20
2025-05-12 16:22:47,961 - INFO -   - Train: 20
2025-05-12 16:22:47,962 - INFO -   - Test: 20
2025-05-12 16:22:47,962 - INFO -   - Test: 20
2025-05-12 16:22:47,962 - INFO -   - Test: 20
2025-05-12 16:22:48,902 - INFO - Model loaded from ./model.eqx
2025-05-12 16:22:48,902 - INFO - Model loaded from ./model.eqx
2025-05-12 16:22:48,902 - INFO - Model loaded from ./model.eqx
2025-05-12 16:23:15,225 - INFO - Evaluation of MSE the test set:
2025-05-12 16:23:15,225 - INFO - Evaluation of MSE the test set:
2025-05-12 16:23:15,225 - INFO - Evaluation of MSE the test set:
2025-05-12 16:23:15,227 - INFO -     - Mean : 0.000137
2025-05-12 16:23:15,227 - INFO -     - Mean : 0.000137
2025-05-12 16:23:15,227 - INFO -     - Mean : 0.000137
2025-05-12 16:23:15,228 - INFO -     - Median : 0.000137
2025-05-12 16:23:15,228 - INFO -     - Median : 0.000137
2025-05-12 16:23:15,228 - INFO -     - Median : 0.000137
2025-05-12 16:23:15,229 - INFO -     - Min : 0.000118
2025-05-12 16:23:15,229 - INFO -     - Min : 0.000118
2025-05-12 16:23:15,229 - INFO -     - Min : 0.000118
2025-05-12 16:23:21,848 - INFO - Finetuning step 0 - Loss: 0.011561
2025-05-12 16:23:21,848 - INFO - Finetuning step 0 - Loss: 0.011561
2025-05-12 16:23:21,848 - INFO - Finetuning step 0 - Loss: 0.011561
2025-05-12 16:23:21,930 - INFO - Finetuning step 100 - Loss: 0.002701
2025-05-12 16:23:21,930 - INFO - Finetuning step 100 - Loss: 0.002701
2025-05-12 16:23:21,930 - INFO - Finetuning step 100 - Loss: 0.002701
2025-05-12 16:23:21,987 - INFO - Finetuning step 200 - Loss: 0.000275
2025-05-12 16:23:21,987 - INFO - Finetuning step 200 - Loss: 0.000275
2025-05-12 16:23:21,987 - INFO - Finetuning step 200 - Loss: 0.000275
2025-05-12 16:23:22,020 - INFO - Finetuning step 255 - Loss: 0.000435
2025-05-12 16:23:22,020 - INFO - Finetuning step 255 - Loss: 0.000435
2025-05-12 16:23:22,020 - INFO - Finetuning step 255 - Loss: 0.000435
2025-05-13 19:07:35,200 - INFO - Started logging to ./artefacts/testing.log
2025-05-13 19:07:35,200 - INFO - Config file: config.yaml
2025-05-13 19:07:35,201 - INFO - ==== Config file's contents ====
2025-05-13 19:07:35,201 - INFO - general:
2025-05-13 19:07:35,201 - INFO -   seed: 2024
2025-05-13 19:07:35,201 - INFO -   train: False
2025-05-13 19:07:35,202 - INFO -   dataset: lorentz63
2025-05-13 19:07:35,202 - INFO -   data_path: ./data/
2025-05-13 19:07:35,202 - INFO -   save_path: None
2025-05-13 19:07:35,203 - INFO - data:
2025-05-13 19:07:35,203 - INFO -   downsample_factor: 1
2025-05-13 19:07:35,203 - INFO -   resolution: [32, 32]
2025-05-13 19:07:35,203 - INFO -   traj_length: 1000
2025-05-13 19:07:35,203 - INFO - model:
2025-05-13 19:07:35,204 - INFO -   mlp_width_size: 48
2025-05-13 19:07:35,204 - INFO -   mlp_depth: 3
2025-05-13 19:07:35,204 - INFO -   activation: swish
2025-05-13 19:07:35,204 - INFO -   input_prev_data: False
2025-05-13 19:07:35,204 - INFO -   model_type: wsm-rnn
2025-05-13 19:07:35,204 - INFO -   nb_rnn_layers: 1
2025-05-13 19:07:35,205 - INFO -   weights_lim: None
2025-05-13 19:07:35,205 - INFO -   apply_tanh_uncertainty: False
2025-05-13 19:07:35,205 - INFO -   time_as_channel: False
2025-05-13 19:07:35,205 - INFO -   forcing_prob: 0.25
2025-05-13 19:07:35,205 - INFO -   noise_theta_init: None
2025-05-13 19:07:35,206 - INFO -   std_lower_bound: 0.0001
2025-05-13 19:07:35,206 - INFO - optimizer:
2025-05-13 19:07:35,206 - INFO -   init_lr: 1e-05
2025-05-13 19:07:35,206 - INFO -   gradients_lim: 1e-07
2025-05-13 19:07:35,206 - INFO -   on_plateau: {'factor': 0.5, 'min_scale': 0.01, 'patience': 20, 'cooldown': 0, 'rtol': 0.0001, 'accum_size': 50}
2025-05-13 19:07:35,206 - INFO - training:
2025-05-13 19:07:35,207 - INFO -   nb_epochs: 2500
2025-05-13 19:07:35,207 - INFO -   batch_size: 1024
2025-05-13 19:07:35,207 - INFO -   print_every: 10
2025-05-13 19:07:35,207 - INFO -   checkpoint_every: 10
2025-05-13 19:07:35,207 - INFO -   nb_recons_loss_steps: None
2025-05-13 19:07:35,208 - INFO -   use_nll_loss: False
2025-05-13 19:07:35,208 - INFO -   inference_start: 100
2025-05-13 19:07:35,893 - INFO - Images shape: (1024, 256, 2)
2025-05-13 19:07:35,894 - INFO - Labels shape: (1024,)
2025-05-13 19:07:35,894 - INFO - Seq length: 256
2025-05-13 19:07:35,894 - INFO - Data size: 2
2025-05-13 19:07:35,895 - INFO - Min/Max in the dataset: (-0.9856310478619905, 0.9731232709978324)
2025-05-13 19:07:35,895 - INFO - Number of batches:
2025-05-13 19:07:35,896 - INFO -   - Train: 20
2025-05-13 19:07:35,896 - INFO -   - Test: 20
2025-05-13 19:07:37,672 - INFO - Model loaded from ./model.eqx
2025-05-13 19:07:59,205 - INFO - Evaluation of MSE the test set:
2025-05-13 19:07:59,206 - INFO -     - Mean : 0.000137
2025-05-13 19:07:59,207 - INFO -     - Median : 0.000137
2025-05-13 19:07:59,207 - INFO -     - Min : 0.000118
2025-05-13 19:08:06,290 - INFO - Finetuning step 0 - Loss: 0.011561
2025-05-13 19:08:06,743 - INFO - Finetuning step 100 - Loss: 0.002701
2025-05-13 19:08:07,042 - INFO - Finetuning step 200 - Loss: 0.000275
2025-05-13 19:08:07,292 - INFO - Finetuning step 255 - Loss: 0.000435
